---
---

@article{geronazzo_strong_2025,
	title = {Strong and weak head-related transfer functions: {The} {eHRTF} analytical framework},
	volume = {5},
	issn = {2691-1191},
	shorttitle = {Strong and weak head-related transfer functions},
	url = {https://doi.org/10.1121/10.0038961},
	doi = {10.1121/10.0038961},
	abstract = {This article introduces an analytical framework for modeling head-related transfer functions (HRTFs) from a listener-centered perspective. The distinction between strong (or general) HRTFs, aiming for idealized physical acoustic fidelity, and weak (or narrow) HRTFs, prioritizing perceptual adequacy in task-specific contexts, frames the contrast in multiple distinctive definitions and scientific methodologies by drawing inspiration from the debate in artificial intelligence. The proposed formalism adopts a Bayesian structure that models HRTFs through a state-space formulation capturing anatomical, contextual, experiential, and task-related factors: the eHRTF. The “e” emphasizes the egocentric perspective, transforming HRTFs from static measurements into mutable auditory representations continuously updated through the listener's feedback. Satisfaction regions are defined in probabilistic terms and characterize how different classes of HRTFs, i.e., individual, generic, super, and personalized, meet perceptual requirements under varying tasks and their complexity.},
	number = {8},
	urldate = {2025-09-08},
	journal = {JASA Express Letters},
	author = {Geronazzo, Michele},
	month = aug,
	year = {2025},
	selected = {true},
	pages = {087202},
}

@article{daugintis_listener_2025,
	title = {Listener {Acoustic} {Personalization} {Challenge}—{LAP24}: {Head}-{Related} {Transfer} {Function} {Dataset} {Harmonization}},
	volume = {6},
	issn = {2644-1322},
	shorttitle = {Listener {Acoustic} {Personalization} {Challenge}—{LAP24}},
	url = {https://ieeexplore.ieee.org/document/11097362},
	doi = {10.1109/OJSP.2025.3592601},
	abstract = {Big data analysis and collation for data-driven head-related transfer function (HRTF) personalization methods are often hindered by systematic differences between HRTF datasets. To address this issue, we designed Task 1 of the inaugural listener acoustic personalisation (LAP) challenge. Researchers were invited to propose strategies for harmonizing HRTFs from a collection of eight different datasets so that dataset-specific artifacts were mitigated while preserving the perceptually relevant attributes of the original HRTFs. Defining the two-sided task required a deeper assessment of the acoustic and perceptual HRTF descriptions to find an evaluation framework that encompassed the two domains. Consequently, a two-stage evaluation was devised to assess the submissions. First, an auditory sound localization model was used to test the perceptual validity of the harmonized HRTFs by estimating the difference in sound localization performance between the original and the harmonized versions. Then, a machine learning classifier was employed to differentiate harmonized HRTF datasets, and its accuracy was used to rank submissions. Three submissions were received, and one was declared a winner according to the evaluation criteria. Further analysis of the submissions revealed some limitations of the evaluation system, prompting a comprehensive review of the task’s inherent complexities. This paper serves as a systematic account of the challenge and relevant considerations, intended to guide future advancements in the field of HRTF personalization research.},
	urldate = {2025-08-17},
	journal = {IEEE Open Journal of Signal Processing},
	author = {Daugintis, Rapolas and Barumerli, Roberto and Geronazzo, Michele and Pauwels, Johan and Picinali, Lorenzo and Poole, Katarina C.},
	year = {2025},
	keywords = {Acoustic measurements, Acoustics, Computational auditory modeling, HRTF dataset, Location awareness, Measurement uncertainty, Numerical models, Rendering (computer graphics), Reviews, Signal processing algorithms, Spatial audio, Transfer functions, head-related transfer function harmonization, machine learning classification},
	pages = {950--964},
}

@article{hogg_listener_2025,
	title = {Listener {Acoustic} {Personalization} {Challenge} - {LAP24}: {Head}-{Related} {Transfer} {Function} {Upsampling}},
	volume = {6},
	issn = {2644-1322},
	shorttitle = {Listener {Acoustic} {Personalization} {Challenge} - {LAP24}},
	url = {https://ieeexplore.ieee.org/document/11078906/authors},
	doi = {10.1109/OJSP.2025.3588776},
	abstract = {Head-related transfer functions (HRTFs) often play a crucial role in spatial hearing, immersive audio applications for virtual reality (VR) and augmented reality (AR), and help in improving hearing assistive devices. The Listener Acoustic Personalisation (LAP) challenge 2024 aimed at advancing research in spatial audio personalisation, with a focus on head-related transfer functions (HRTFs). The challenge was split into two tasks: Task 1 was on HRTF harmonisation, and Task 2 dealt with spatial HRTF upsampling. This paper presents the results and reports the findings related to Task 2 of the LAP challenge. The submissions to Task 2 employed both algorithmic and machine learning-based approaches, which were evaluated on three key spatial audio objective metrics, including the log-spectral distortion (LSD), interaural time difference (ITD), and the interaural level difference (ILD). The results highlighted the strengths and limitations of various upsampling techniques, with learning-based methods demonstrating superior performance at lower sparsity levels. In terms of the LSD, seven of the submissions achieved an impressive performance of less than 5 dB when upsampling from only three measurement points. The results also highlighted that most submissions were often not able to outperform a generic HRTF created by averaging the HRTFs in the training dataset. One of the main contributions of this paper is that it showcases the limitations of objective metrics when it comes to evaluating HRTF upsampling. Therefore, this paper argues that a more holistic approach is needed going forward, which should include the integration of multiple perceptually relevant measures, as this is the only way to ensure a well-rounded assessment of HRTF upsampling quality.},
	urldate = {2025-08-17},
	journal = {IEEE Open Journal of Signal Processing},
	author = {Hogg, Aidan O. T. and Barumerli, Roberto and Daugintis, Rapolas and Poole, Katarina C. and Brinkmann, Fabian and Picinali, Lorenzo and Geronazzo, Michele},
	year = {2025},
	keywords = {Acoustics, Computational auditory modelling, Computational modeling, Ear, Immersive audio, Interpolation, Measurement, Psychoacoustic models, Signal processing algorithms, Spatial audio, Transfer functions, head-related transfer function, upsampling},
	pages = {926--941},
}

@article{fantini_survey_2025,
	title = {A {Survey} on {Machine} {Learning} {Techniques} for {Head}-{Related} {Transfer} {Function} {Individualization}},
	volume = {6},
	issn = {2644-1322},
	url = {https://ieeexplore.ieee.org/document/10836943},
	doi = {10.1109/OJSP.2025.3528330},
	abstract = {Machine learning (ML) has become pervasive in various research fields, including binaural synthesis personalization, which is crucial for sound in immersive virtual environments. Researchers have mainly addressed this topic by estimating the individual head-related transfer function (HRTF). HRTFs are utilized to render audio signals at specific spatial positions, thereby simulating real-world sound wave interactions with the human body. As such, an HRTF that is compliant with individual characteristics enhances the realism of the binaural simulation. This survey systematically examines the HRTF individualization works based on ML proposed in the literature. The analyzed works are organized according to the processing steps involved in the ML workflow, including the employed dataset, input and output types, data preprocessing operations, ML models, and model evaluation. In addition to categorizing the works of the existing literature, this survey discusses their achievements, identifies their limitations, and outlines aspects that require further investigation at the crossroads of research communities in acoustics, audio signal processing, and machine learning.},
	urldate = {2025-02-04},
	journal = {IEEE Open Journal of Signal Processing},
	author = {Fantini, Davide and Geronazzo, Michele and Avanzini, Federico and Ntalampiras, Stavros},
	year = {2025},
	selected = {true},
	keywords = {Acoustic measurements, Azimuth, Ear, HRTF individualization, Machine learning, Magnetic heads, Numerical simulation, Spatial audio, Surveys, Torso, Transfer functions, binaural synthesis, machine learning, spatial audio},
	pages = {30--56},
}

@article{privitera_role_2024,
	title = {The {Role} of {Audio} in {Immersive} {Storytelling}: a {Systematic} {Review} in {Cultural} {Heritage}},
	issn = {1573-7721},
	shorttitle = {The {Role} of {Audio} in {Immersive} {Storytelling}},
	url = {https://doi.org/10.1007/s11042-024-19288-4},
	doi = {10.1007/s11042-024-19288-4},
	abstract = {Nowadays, Virtual and Augmented Reality technologies play a supportive role in many research fields. In cultural heritage, various examples are available, including storytelling and narratives, where they can provide immersive and enhanced experiences to visitors and tourists, especially for entertainment and educational purposes. This review aims to investigate the opportunities that soundscape design and advanced sonic interactions in virtual and augmented environments can bring to cultural heritage sites and museums in terms of presence, emotional content, and cultural dissemination. Nineteen-two papers have been identified through the PRISMA methodology, and a promising positive effect of sonic interaction on user experience in a virtual environment can be observed in various studies, notwithstanding a general lack of specific contributions on the use of sound rendering and audio spatialisation for improving such experiences. Moreover, this work identifies the main involved research areas and discusses the state-of-the-art best practices and case studies where sonic interactions may assume a central role. The final part suggests possible future directions and applications for more engaging and immersive storytelling in the cultural heritage domain.},
	language = {en},
	urldate = {2024-06-26},
	journal = {Multimedia Tools and Applications},
	author = {Privitera, Alessandro Giuseppe and Fontana, Federico and Geronazzo, Michele},
	month = jun,
	year = {2024},
	selected = {true},
	keywords = {Cultural heritage, Immersive audio, Sonic interaction design, Storytelling, Virtual/augmented reality},
}

@article{fantini_co-immersion_2023,
	title = {Co-immersion in {Audio} {Augmented} {Virtuality}: the {Case} {Study} of a {Static} and {Approximated} {Late} {Reverberation} {Algorithm}},
	issn = {1941-0506},
	shorttitle = {Co-immersion in {Audio} {Augmented} {Virtuality}},
	url = {https://ieeexplore.ieee.org/document/10269056},
	doi = {10.1109/TVCG.2023.3320213},
	abstract = {In immersive Audio Augmented Reality, a virtual sound source should be indistinguishable from the existing real ones. This property can be evaluated with the co-immersion criterion, which encompasses scenes constituted by arbitrary configurations of real and virtual objects. Thus, we introduce the term Audio Augmented Virtuality (AAV) to describe a fully virtual environment consisting of auditory content captured from the real world, augmented by synthetic sound generation. We propose an experimental design in AAV investigating how simplified late reverberation (LR) affects the co-immersion of a sound source. Participants listened to simultaneous virtual speakers dynamically rendered through spatial Room Impulse Responses, and were asked to detect the presence of an impostor, i.e., a speaker rendered with one of two simplified LR conditions. Detection rates were found to be close to chance level, especially for one condition, suggesting a limited influence on co-immersion of the simplified LR in the evaluated AAV scenes. This methodology can be straightforwardly extended and applied to different acoustics scenes, complexities, i.e., the number of simultaneous speakers, and rendering parameters in order to further investigate the requirements for immersive audio technologies in AAR and AAV applications.},
	urldate = {2023-10-04},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Fantini, Davide and Presti, Giorgio and Geronazzo, Michele and Bona, Riccardo and Privitera, Alessandro Giuseppe and Avanzini, Federico},
	year = {2023},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	pages = {1--11},
}

@article{geronazzo_shaping_2023,
	title = {Shaping the auditory peripersonal space with motor planning in immersive virtual reality},
	issn = {1434-9957},
	url = {https://doi.org/10.1007/s10055-023-00854-4},
	doi = {10.1007/s10055-023-00854-4},
	abstract = {Immersive audio technologies require personalized binaural synthesis through headphones to provide perceptually plausible virtual and augmented reality (VR/AR) simulations. We introduce and apply for the first time in VR contexts the quantitative measure called premotor reaction time (pmRT) for characterizing sonic interactions between humans and the technology through motor planning. In the proposed basic virtual acoustic scenario, listeners are asked to react to a virtual sound approaching from different directions and stopping at different distances within their peripersonal space (PPS). PPS is highly sensitive to embodied and environmentally situated interactions, anticipating the motor system activation for a prompt preparation for action. Since immersive VR applications benefit from spatial interactions, modeling the PPS around the listeners is crucial to reveal individual behaviors and performances. Our methodology centered around the pmRT is able to provide a compact description and approximation of the spatiotemporal PPS processing and boundaries around the head by replicating several well-known neurophysiological phenomena related to PPS, such as auditory asymmetry, front/back calibration and confusion, and ellipsoidal action fields.},
	language = {en},
	urldate = {2023-10-10},
	journal = {Virtual Reality},
	author = {Geronazzo, Michele and Barumerli, Roberto and Cesari, Paola},
	month = oct,
	year = {2023},
	selected = {true},
	keywords = {Immersive audio, Motor planning, Peripersonal space, Spatial audio rendering, Virtual reality},
}

@article{barumerli_bayesian_2023,
	title = {A {Bayesian} model for human directional localization of broadband static sound sources},
	volume = {7},
	copyright = {© The Author(s), Published by EDP Sciences, 2023},
	issn = {2681-4617},
	url = {https://acta-acustica.edpsciences.org/articles/aacus/abs/2023/01/aacus210056/aacus210056.html},
	doi = {10.1051/aacus/2023006},
	abstract = {Humans estimate sound-source directions by combining prior beliefs with sensory evidence. Prior beliefs represent statistical knowledge about the environment, and the sensory evidence consists of auditory features such as interaural disparities and monaural spectral shapes. Models of directional sound localization often impose constraints on the contribution of these features to either the horizontal or vertical dimension. Instead, we propose a Bayesian model that flexibly incorporates each feature according to its spatial precision and integrates prior beliefs in the inference process. The model estimates the direction of a single, broadband, stationary sound source presented to a static human listener in an anechoic environment. We simplified interaural features to be broadband and compared two model variants, each considering a different type of monaural spectral features: magnitude profiles and gradient profiles. Both model variants were fitted to the baseline performance of five listeners and evaluated on the effects of localizing with non-individual head-related transfer functions (HRTFs) and sounds with rippled spectrum. We found that the variant equipped with spectral gradient profiles outperformed other localization models. The proposed model appears particularly useful for the evaluation of HRTFs and may serve as a basis for future extensions towards modeling dynamic listening conditions.},
	language = {en},
	urldate = {2023-05-02},
	journal = {Acta Acustica},
	author = {Barumerli, Roberto and Majdak, Piotr and Geronazzo, Michele and Meijer, David and Avanzini, Federico and Baumgartner, Robert},
	year = {2023},
	note = {Publisher: EDP Sciences},
	pages = {12},
}

@article{picinali_sonicom_2022,
	title = {The {SONICOM} {Project}: {Artificial} {Intelligence}-{Driven} {Immersive} {Audio}, {From} {Personalization} to {Modeling} [{Applications} {Corner}]},
	volume = {39},
	issn = {1558-0792},
	shorttitle = {The {SONICOM} {Project}},
	doi = {10.1109/MSP.2022.3182929},
	abstract = {Every individual perceives spatial audio differently, due in large part to the unique and complex shape of ears and head. Therefore, high-quality, headphone-based spatial audio should be uniquely tailored to each listener in an effective and efficient manner. Artificial intelligence (AI) is a powerful tool that can be used to drive forward research in spatial audio personalization. The SONICOM project aims to employ a data-driven approach that links physiological characteristics of the ear to the individual acoustic filters, which allows us to localize sound sources and perceive them as being located around us. A small amount of data acquired from users could allow personalized audio experiences, and AI could facilitate this by offering a new perspective on the matter. A Bayesian approach to computational neuroscience and binaural sound reproduction will be linked to create a metric for AI-based algorithms that will predict realistic spatial audio quality. Being able to consistently and repeatedly evaluate and quantify the improvements brought by technological advancements, as well as the impact these have on complex interactions in virtual environments, will be key for the development of new techniques and for unlocking new approaches to understanding the mechanisms of human spatial hearing and communication.},
	number = {6},
	journal = {IEEE Signal Processing Magazine},
	author = {Picinali, Lorenzo and Katz, Brian FG and Geronazzo, Michele and Majdak, Piotr and Reyes-Lecuona, Arcadio and Vinciarelli, Alessandro},
	month = nov,
	year = {2022},
	selected = {true},
	note = {Conference Name: IEEE Signal Processing Magazine},
	keywords = {Artificial intelligence, Bayes methods, Computational neuroscience, Data models, Physiology, Predictive models, Spatial audio, Standardization, Virtual environments},
	pages = {85--88},
}

@article{bahadori_action_2021,
	title = {Action planning and affective states within the auditory peripersonal space in normal hearing and cochlear-implanted listeners},
	volume = {155},
	issn = {0028-3932},
	url = {https://www.sciencedirect.com/science/article/pii/S0028393221000415},
	doi = {10.1016/j.neuropsychologia.2021.107790},
	abstract = {Fast reaction to approaching stimuli is vital for survival. When sounds enter the auditory peripersonal space (PPS), sounds perceived as being nearer elicit higher motor cortex activation. There is a close relationship between motor preparation and the perceptual components of sounds, particularly of highly arousing sounds. Here we compared the ability to recognize, evaluate, and react to affective stimuli entering the PPS between 20 normal-hearing (NH, 7 women) and 10 cochlear-implanted (CI, 3 women) subjects. The subjects were asked to quickly flex their arm in reaction to positive (P), negative (N), and neutral (Nu) affective sounds ending virtually at five distances from their body. Pre-motor reaction time (pm-RT) was detected via electromyography from the postural muscles to measure action anticipation at the sound-stopping distance; the sounds were also evaluated for their perceived level of valence and arousal. While both groups were able to localize sound distance, only the NH group modulated their pm-RT based on the perceived sound distance. Furthermore, when the sound carried no affective components, the pm-RT to the Nu sounds was shorter compared to the P and the N sounds for both groups. Only the NH group perceived the closer sounds as more arousing than the distant sounds, whereas both groups perceived sound valence similarly. Our findings underline the role of emotional states in action preparation and describe the perceptual components essential for prompt reaction to sounds approaching the peripersonal space.},
	language = {en},
	urldate = {2021-04-06},
	journal = {Neuropsychologia},
	author = {Bahadori, Mehrdad and Barumerli, Roberto and Geronazzo, Michele and Cesari, Paola},
	month = may,
	year = {2021},
	keywords = {Action planning, Affective states, Distance estimation, Emotion perception, Peripersonal space, cochlear implant},
}

@article{prepelita_pinna-related_2020,
	title = {Pinna-related transfer functions and lossless wave equation using finite-difference methods: {Validation} with measurements},
	volume = {147},
	issn = {0001-4966},
	shorttitle = {Pinna-related transfer functions and lossless wave equation using finite-difference methods},
	doi = {10.1121/10.0001230},
	abstract = {Nowadays, wave-based simulations of head-related transfer functions (HRTFs) lack strong justifications to replace HRTF measurements. The main cause is the complex interactions between uncertainties and biases in both simulated and measured HRTFs. This paper deals with the validation of pinna-related high-frequency information in the ipsilateral directions-of-arrival, computed by lossless wave-based simulations with finite-difference models. A simpler yet related problem is given by the pinna-related transfer function (PRTF), which encodes the acoustical effects of only the external ear. Results stress that PRTF measurements are generally highly repeatable but not necessarily easily reproducible, leading to critical issues in terms of reliability for any ground truth condition. On the other hand, PRTF simulations exhibit an increasing uncertainty with frequency and grid-dependent frequency changes, which are here quantified analyzing the benefits in the use of a unique asymptotic solution. In this validation study, the employed finite-difference model accurately and reliably predict the PRTF magnitude mostly within ±1 dB up to ≈{\textless}math display="inline" overflow="scroll" altimg="eq-00001.gif"{\textgreater} {\textless}mo{\textgreater}≈{\textless}/mo{\textgreater}{\textless}/math{\textgreater}8 kHz and a space- and frequency-averaged spectral distortion within about 2 dB up to ≈{\textless}math display="inline" overflow="scroll" altimg="eq-00002.gif"{\textgreater} {\textless}mo{\textgreater}≈{\textless}/mo{\textgreater}{\textless}/math{\textgreater} 18 kHz.},
	number = {5},
	urldate = {2020-05-26},
	journal = {The Journal of the Acoustical Society of America},
	author = {Prepeliță, Sebastian T. and Gómez Bolaños, Javier and Geronazzo, Michele and Mehra, Ravish and Savioja, Lauri},
	month = may,
	year = {2020},
	note = {Publisher: Acoustical Society of America},
	pages = {3631--3645},
}

@article{geronazzo_superhuman_2020,
	title = {Superhuman {Hearing} - {Virtual} {Prototyping} of {Artificial} {Hearing}: a {Case} {Study} on {Interactions} and {Acoustic} {Beamforming}},
	volume = {26},
	issn = {1941-0506},
	shorttitle = {Superhuman {Hearing} - {Virtual} {Prototyping} of {Artificial} {Hearing}},
	doi = {10.1109/TVCG.2020.2973059},
	abstract = {Directivity and gain in microphone array systems for hearing aids or hearable devices allow users to acoustically enhance the information of a source of interest. This source is usually positioned directly in front. This feature is called acoustic beamforming. The current study aimed to improve users' interactions with beamforming via a virtual prototyping approach in immersive virtual environments (VEs). Eighteen participants took part in experimental sessions composed of a calibration procedure and a selective auditory attention voice-pairing task. Eight concurrent speakers were placed in an anechoic environment in two virtual reality (VR) scenarios. The scenarios were a purely virtual scenario and a realistic 360° audio-visual recording. Participants were asked to find an individual optimal parameterization for three different virtual beamformers: (i) head-guided, (ii) eye gaze-guided, and (iii) a novel interaction technique called dual beamformer, where head-guided is combined with an additional hand-guided beamformer. None of the participants were able to complete the task without a virtual beamformer (i.e., in normal hearing condition) due to the high complexity introduced by the experimental design. However, participants were able to correctly pair all speakers using all three proposed interaction metaphors. Providing superhuman hearing abilities in the form of a dual acoustic beamformer guided by head and hand movements resulted in statistically significant improvements in terms of pairing time, suggesting the task-relevance of interacting with multiple points of interests.},
	number = {5},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Geronazzo, Michele and Vieira, Luis S. and Nilsson, Niels Christian and Udesen, Jesper and Serafin, Stefania},
	month = may,
	year = {2020},
	selected = {true},
	keywords = {Acoustic beamforming, Acoustics, Array signal processing, Artificial hearing, Auditory system, Ear, Microphones, Multi-speaker scenario, Sonic interactions, Task analysis, Virtual Reality, Virtual prototyping, hearing aids},
	pages = {1912--1922},
}

@article{serafin_reflections_2020,
	title = {Reflections from five years of {Sonic} {Interactions} in {Virtual} {Environments} workshops},
	volume = {49},
	issn = {0929-8215},
	url = {https://doi.org/10.1080/09298215.2019.1708413},
	doi = {10.1080/09298215.2019.1708413},
	abstract = {For the past five years, the authors have been running at the IEEE Virtual Reality Conference a Workshop called Sonic Interactions in Virtual Environments (SIVE). The main goal of the workshop series has been to increase among the virtual reality community awareness of the importance of sonic elements when designing multimodal and immersive virtual environments. Starting from this experience, this paper presents a survey of the main active research topics related to sound in virtual and augmented reality (VR/AR), ranging from basic research in spatial audio rendering and sonic interaction design to applications in interactive environments for training, health, rehabilitation, entertainment, and art. Looking at the different research topics emerging from laboratories worldwide, the paper discusses how different research communities can collaborate and benefit from each other in order to increase sound awareness in VR and AR.},
	number = {1},
	urldate = {2020-01-31},
	journal = {Journal of New Music Research},
	author = {Serafin, Stefania and Avanzini, Federico and Goetzen, Amalia De and Erkut, Cumhur and Geronazzo, Michele and Grani, Francesco and Nilsson, Niels Christian and Nordahl, Rolf},
	month = jan,
	year = {2020},
	keywords = {Sonic interaction, Virtual Reality, spatial audio rendering},
	pages = {24--34},
}

@article{gabrieli_cervical_2019,
	title = {Cervical {Spine} {Motion} {During} {Vehicle} {Extrication} of {Healthy} {Volunteers}},
	copyright = {© 2019 National Association of EMS Physicians},
	url = {https://www.tandfonline.com/doi/abs/10.1080/10903127.2019.1695298},
	doi = {10.1080/10903127.2019.1695298},
	abstract = {(2019). Cervical Spine Motion During Vehicle Extrication of Healthy Volunteers. Prehospital Emergency Care. Ahead of Print.},
	language = {en},
	urldate = {2019-12-20},
	journal = {Prehospital Emergency Care},
	author = {Gabrieli, Alberto and Nardello, Francesca and Geronazzo, Michele and Marchetti, Pierpaolo and Liberto, Alessandro and Arcozzi, Daniele and Polati, Enrico and Cesari, Paola and Zamparo, Paola},
	month = dec,
	year = {2019},
}

@article{prepelita_pinna-related_2019,
	title = {Pinna-related transfer functions and lossless wave equation using finite-difference methods: {Verification} and asymptotic solution},
	volume = {146},
	issn = {0001-4966},
	shorttitle = {Pinna-related transfer functions and lossless wave equation using finite-difference methods},
	doi = {10.1121/1.5131245},
	abstract = {A common approach when employing discrete mathematical models is to assess the reliability and credibility of the computation of interest through a process known as solution verification. Present-day computed head-related transfer functions (HRTFs) seem to lack robust and reliable assessments of the numerical errors embedded in the results which makes validation of wave-based models difficult. This process requires a good understanding of the involved sources of error which are systematically reviewed here. The current work aims to quantify the pinna-related high-frequency computational errors in the context of HRTFs and wave-based simulations with finite-difference models. As a prerequisite for solution verification, code verification assesses the reliability of the proposed implementation. In this paper, known and manufactured formal solutions are used and tailored for the wave equation and frequency-independent boundary conditions inside a rectangular room of uniform acoustic wall-impedance. Asymptotic estimates for pinna acoustics are predicted in the frequency domain based on regression models and a convergence study on sub-millimeter grids. Results show an increasing uncertainty with frequency and a significant frequency-dependent change among computations on different grids.},
	number = {5},
	urldate = {2019-11-27},
	journal = {The Journal of the Acoustical Society of America},
	author = {Prepeliță, Sebastian T. and Gómez Bolaños, Javier and Geronazzo, Michele and Mehra, Ravish and Savioja, Lauri},
	month = nov,
	year = {2019},
	pages = {3629--3645},
}

@article{geronazzo_creating_2019,
	title = {Creating an {Audio} {Story} with {Interactive} {Binaural} {Rendering} in {Virtual} {Reality}},
	volume = {2019},
	doi = {10.1155/2019/1463204},
	abstract = {The process of listening to an audiobook is usually a rather passive act that does not require an active interaction. If spatial interaction is incorporated into a storytelling scenario, can open. Possibilities of a novel experience which allows an active participation might affect the user-experience. The aim of this paper is to create a portable prototype system based on an embedded hardware platform, allowing listeners to get immersed in an interactive audio storytelling experience enhanced by dynamic binaural audio rendering. For the evaluation of the experience, a short story based on the horror narrative of Stephen King’s Strawberry Springs is adapted and designed in virtual environments. A comparison among three different listening experiences, namely, (i) monophonic (traditional audio story), (ii) static binaural rendering (state-of-the-art audio story), and (iii) our prototype, is conducted. We discuss the quality of the experience based on usability testing, physiological data, emotional assessments, and questionnaires for immersion and spatial presence. Results identify a clear trend for an increase in immersion with our prototype compared to traditional audiobooks, showing also an emphasis on story-specific emotions, i.e., terror and fear.},
	language = {en},
	urldate = {2019-11-15},
	journal = {Wireless Communications and Mobile Computing},
	author = {Geronazzo, Michele and Rosenkvist, Amalie and Eriksen, David Sebastian and Markmann-Hansen, Camilla Kirstine and Køhlert, Jeppe and Valimaa, Miicha and Vittrup, Mikkel Brogaard and Serafin, Stefania},
	year = {2019},
	pages = {1--14},
}

@article{degli_innocenti_mobile_2019,
	title = {Mobile virtual reality for musical genre learning in primary education},
	volume = {139},
	issn = {0360-1315},
	doi = {10.1016/j.compedu.2019.04.010},
	abstract = {Mobile virtual reality (VR) is increasingly becoming popular and accessible to everyone that holds a smartphone. In particular, digital didactics can take advantage of natural interaction and immersion in virtual environments, starting from primary education. This paper investigates the problem of enhancing music learning in primary education through the use of mobile VR. To this end, technical and methodological frameworks were developed, and were tested with two classes in the last year of a primary school (10 years old children). The classes were involved in an evaluation study on music genre identification and learning with a multi-platform mobile application called VR4EDU. Students were immersed in music performances of different genres (e.g., classical, country, jazz, and swing), navigating inside several musical rooms. The evaluation of the didactic protocol shows a statistically significant improvement in learning genre characterization (i.e., typical instruments and their spatial arrangements on stage) compared to traditional lessons with printed materials and passive listening. These results show that the use of mobile VR technologies in synergy with traditional teaching methodologies can improve the music learning experience in primary education, in terms of active listening, attention, and time. The inclusion of pupils with certified special needs strengthened our results.},
	urldate = {2019-05-29},
	journal = {Computers \& Education},
	author = {Degli Innocenti, Edoardo and Geronazzo, Michele and Vescovi, Diego and Nordahl, Rolf and Serafin, Stefania and Ludovico, Luca Andrea and Avanzini, Federico},
	month = oct,
	year = {2019},
	keywords = {Mobile virtual reality, Music genre learning, Music primary education, Spatial audio, navigation},
	pages = {102--117},
}

@article{geronazzo_applying_2019,
	title = {Applying a {Single}-{Notch} {Metric} to {Image}-{Guided} {Head}-{Related} {Transfer} {Function} {Selection} for {Improved} {Vertical} {Localization}},
	volume = {67},
	number = {6},
	journal = {J\_AES},
	author = {Geronazzo, Michele and Peruch, Enrico and Prandoni, Fabio and Avanzini, Federico},
	year = {2019},
	note = {Publisher: Audio Engineering Society},
	pages = {414--428},
}

@article{andreasen_auditory_2019,
	title = {Auditory feedback for navigation with echoes in virtual environments: training procedure and orientation strategies},
	volume = {25},
	issn = {1077-2626},
	shorttitle = {Auditory feedback for navigation with echoes in virtual environments},
	doi = {10.1109/TVCG.2019.2898787},
	abstract = {Being able to hear objects in an environment, for example using echolocation, is a challenging task. The main goal of the current work is to use virtual environments (VEs) to train novice users to navigate using echolocation. Previous studies have shown that musicians are able to differentiate sound pulses from reflections. This paper presents design patterns for VE simulators for both training and testing procedures, while classifying users' navigation strategies in the VE. Moreover, the paper presents features that increase users' performance in VEs. We report the findings of two user studies: a pilot test that helped improve the sonic interaction design, and a primary study exposing participants to a spatial orientation task during four conditions which were early reflections (RF), late reverberation (RV), early reflections-reverberation (RR) and visual stimuli (V). The latter study allowed us to identify navigation strategies among the users. Some users (10/26) reported an ability to create spatial cognitive maps during the test with auditory echoes, which may explain why this group performed better than the remaining participants in the RR condition.},
	number = {5},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Andreasen, A. and Geronazzo, M. and Nilsson, N. C. and Zovnercuka, J. and Konovalov, K. and Serafin, S.},
	month = may,
	year = {2019},
	keywords = {Auditory system, Human echolocation, Reverberation, Task analysis, Training, Virtual Reality, Visualization, binaural synthesis, navigation, sonic interactions, spatial audio, spatial cognition},
	pages = {1876--1886},
}

@article{geronazzo_we_2018,
	title = {Do we need individual head-related transfer functions for vertical localization? {The} case study of a spectral notch distance metric},
	volume = {26},
	issn = {2329-9290},
	shorttitle = {Do we need individual head-related transfer functions for vertical localization?},
	doi = {10.1109/TASLP.2018.2821846},
	abstract = {This paper deals with the issue of individualizing the head-related transfer function (HRTF) rendering process for auditory elevation perception. Is it possible to find a nonindividual, personalized HRTF set that allows a listener to have an equally accurate localization performance than with his/her individual HRTFs? We propose a psychoacoustically motivated, anthropometry based mismatch function between HRTF pairs that exploits the close relation between the listener's pinna geometry and localization cues. This is evaluated using an auditory model that computes a mapping between HRTF spectra and perceived spatial locations. Results on a large number of subjects in the center for image processing and integrated computing (CIPIC) and acoustics research institute (ARI) HRTF databases suggest that there exists a nonindividual HRTF set, which allows a listener to have an equally accurate vertical localization than with individual HRTFs. Furthermore, we find the optimal parameterization of the proposed mismatch function, i.e., the one that best reflects the information given by the auditory model. Our findings show that the selection procedure yields statistically significant improvements with respect to dummy-head HRTFs or random HRTF selection, with potentially high impact from an applicative point of view.},
	number = {7},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Geronazzo, Michele and Spagnol, Simone and Avanzini, Federico},
	month = jul,
	year = {2018},
	selected = {true},
	keywords = {Acoustics, Computational modeling, Databases, HRTF selection, Psychoacoustic models, Spatial audio, Speech processing, Transfer functions, auditory models, head-related transfer functions (HRTFs), individualized HRTFs, spectral notch metric, speech, vertical localization},
	pages = {1243--1256},
}

@article{serafin_sonic_2018,
	title = {Sonic interactions in virtual reality: state of the art, current challenges and future directions},
	volume = {38},
	issn = {0272-1716},
	shorttitle = {Sonic interactions in virtual reality},
	doi = {10.1109/MCG.2018.193142628},
	abstract = {A high fidelity but efficient sound simulation is an essential element of any virtual reality experience. In recent years, several advances in hardware and software technologies are facilitating the development of immersive interactive sound rendering experiences. In this paper we present a review of the state of the art of such simulations, with a focus on the different elements such as physics based simulation of sound effects and their propagation in space to binaural rendering. We present how the different elements of the sound design pipeline have been addressed in the literature, trying to find the trade-off between accuracy and plausibility. Recent applications and current challenges are also presented.},
	number = {2},
	journal = {IEEE Computer Graphics and Applications},
	author = {Serafin, Stefania and Geronazzo, Michele and Nilsson, Niels Christian and Erkut, Cumhur and Nordahl, Rolf},
	year = {2018},
	keywords = {Information Interfaces and Representation (HCI), Information Technology and Systems, Object recognition, Sound and Music Computing, immersive sound, sonic interactions},
	pages = {31--43},
}

@article{geronazzo_auditory_2016,
	title = {Auditory navigation with a tubular acoustic model for interactive distance cues and personalized head-related transfer functions},
	volume = {10},
	issn = {1783-7677, 1783-8738},
	shorttitle = {J. on {Multimodal} {User} {Interfaces}},
	url = {http://link.springer.com/article/10.1007/s12193-016-0221-z},
	doi = {10.1007/s12193-016-0221-z},
	abstract = {This paper presents a novel spatial auditory display that combines a virtual environment based on a Digital Waveguide Mesh (DWM) model of a small tubular shape with a binaural rendering system with personalized head-related transfer functions (HRTFs) allowing interactive selection of absolute 3D spatial cues of direction as well as egocentric distance. The tube metaphor in particular minimizes loudness changes with distance, providing mainly direct-to-reverberant and spectral cues. The proposed display was assessed through a target-reaching task where participants explore a 2D virtual map with a pen tablet and hit a sound source (the target) using auditory information only; subjective time to hit and traveled distance were analyzed for three experiments. The first one aimed at assessing the proposed HRTF selection method for personalization and dimensionality of the reaching task, with particular attention to elevation perception; we showed that most subjects performed better when they had to reach a vertically unbounded (2D) rather then an elevated (3D) target. The second experiment analyzed interaction between the tube metaphor and HRTF showing a dominant effect of DWM model over binaural rendering. In the last experiment, participants using absolute distance cues from the tube model performed comparably well to when they could rely on more robust, although relative, intensity cues. These results suggest that participants made proficient use of both binaural and reverberation cues during the task, displayed as part of a coherent 3D sound model, in spite of the known complexity of use of both such cues. HRTF personalization was beneficial for participants who were able to perceive vertical dimension of a virtual sound. Further work is needed to add full physical consistency to the proposed auditory display.},
	language = {en},
	number = {3},
	urldate = {2016-05-21},
	journal = {Journal on Multimodal User Interfaces},
	author = {Geronazzo, Michele and Avanzini, Federico and Fontana, Federico},
	month = sep,
	year = {2016},
	keywords = {Auditory distance rendering, Digital waveguide mesh, Head-related transfer function, Human spatial navigation, Perceptual model individualization, Signal, Image and Speech Processing, Target-reaching task, auditory displays, hci},
	pages = {273--284},
}

@article{prepelita_influence_2016,
	title = {Influence of voxelization on finite difference time domain simulations of head-related transfer functions},
	volume = {139},
	issn = {0001-4966},
	url = {http://scitation.aip.org/content/asa/journal/jasa/139/5/10.1121/1.4947546},
	doi = {10.1121/1.4947546},
	abstract = {The scattering around the human pinna that is captured by the Head-Related Transfer Functions (HRTFs) is a complex problem that creates uncertainties in both acoustical measurements and simulations. Within the simulation framework of Finite Difference Time Domain(FDTD) with axis-aligned staircase boundaries resulting from a voxelization process, the voxelization-based uncertainty propagating in the HRTF-captured sound field is quantified for one solid and two surface voxelization algorithms. Simulated results utilizing a laser-scanned mesh of Knowles Electronics Manikin for Acoustic Research (KEMAR) show that in the context of complex geometries with local topology comparable to grid spacing such as the human pinna, the voxelization-related uncertainties in simulations emerge at lower frequencies than the generally used accuracy bandwidths. Numerical simulations show that the voxelization process induces both random error and algorithm-dependent bias in the simulated HRTF spectral features. Frequencies fr below which the random error is bounded by various dB thresholds are estimated and predicted. Particular shortcomings of the used voxelization algorithms are identified and the influence of the surface impedance on the induced errors is studied. Simulations are also validated against measurements.},
	number = {5},
	urldate = {2016-05-10},
	journal = {The Journal of the Acoustical Society of America},
	author = {Prepeliță, Sebastian and Geronazzo, Michele and Avanzini, Federico and Savioja, Lauri},
	month = may,
	year = {2016},
	keywords = {Acoustic waves, Acoustical measurements, Dispersion, Finite difference time domain calculations, Frequency analyzers},
	pages = {2489--2504},
}

@article{turchet_localization_2016,
	title = {Localization of self-generated synthetic footstep sounds on different walked-upon materials through headphones},
	volume = {20},
	issn = {1359-4338, 1434-9957},
	url = {http://link.springer.com/article/10.1007/s10055-015-0272-6},
	doi = {10.1007/s10055-015-0272-6},
	abstract = {This paper focuses on the localization of footstep sounds interactively generated during walking and provided through headphones. Three distinct experiments were conducted in a laboratory involving a pair of sandals enhanced with pressure sensors and a footstep synthesizer capable of simulating two typologies of surface materials: solid (e.g., wood) and aggregate (e.g., gravel). Different sound delivery methods (mono, stereo, binaural) as well as several surface materials, in the presence or absence of concurrent contextual auditory information provided as soundscapes, were evaluated in a vertical localization task. Results showed that solid surfaces were localized significantly farther from the walker’s feet than the aggregate ones. This effect was independent of the used rendering technique, of the presence of soundscapes, and of merely temporal or spectral attributes of sound. The effect is hypothesized to be due to a semantic conflict between auditory and haptic information such that the higher the semantic incongruence the greater the distance of the perceived sound source from the feet. The presented results contribute to the development of further knowledge toward a basis for the design of continuous multimodal feedback in virtual reality applications .},
	language = {en},
	number = {1},
	urldate = {2016-02-15},
	journal = {Virtual Reality},
	author = {Turchet, Luca and Spagnol, Simone and Geronazzo, Michele and Avanzini, Federico},
	month = mar,
	year = {2016},
	keywords = {Artificial Intelligence (incl. Robotics), Computer Graphics, Computing Methodologies, Image Processing and Computer Vision, Interactive auditory feedback, Walking, localization},
	pages = {1--16},
}

@article{geronazzo_interactive_2016,
	series = {Data {Sonification} and {Sound} {Design} in {Interactive} {Systems}},
	title = {Interactive spatial sonification for non-visual exploration of virtual maps},
	volume = {85},
	issn = {1071-5819},
	url = {http://www.sciencedirect.com/science/article/pii/S1071581915001287},
	doi = {10.1016/j.ijhcs.2015.08.004},
	abstract = {This paper presents a multimodal interactive system for non-visual (auditory-haptic) exploration of virtual maps. The system is able to display haptically the height profile of a map, through a tactile mouse. Moreover, spatial auditory information is provided in the form of virtual anchor sounds located in specific points of the map, and delivered through headphones using customized Head-Related Transfer Functions (HRTFs). The validity of the proposed approach is investigated through two experiments on non-visual exploration of virtual maps. The first experiment has a preliminary nature and is aimed at assessing the effectiveness and the complementarity of auditory and haptic information in a goal reaching task. The second experiment investigates the potential of the system in providing subjects with spatial knowledge: specifically in helping with the construction of a cognitive map depicting simple geometrical objects. Results from both experiments show that the proposed concept, design, and implementation allow to effectively exploit the complementary natures of the “proximal” haptic modality and the “distal” auditory modality. Implications for orientation \&amp; mobility (O\&amp;M) protocols for visually impaired subjects are discussed.},
	urldate = {2015-11-05},
	journal = {International Journal of Human-Computer Studies},
	author = {Geronazzo, Michele and Bedin, Alberto and Brayda, Luca and Campus, Claudio and Avanzini, Federico},
	month = jan,
	year = {2016},
	selected = {true},
	keywords = {3D audio, Binaural sound, Haptic mouse, Multisensory integration, Non-visual navigation, Virtual maps, haptics, multimodal interaction, visual impairment},
	pages = {4--15},
}

@article{geronazzo_absence_2015,
	title = {Absence of modulatory action on haptic height perception with musical pitch},
	volume = {6},
	issn = {1664-1078},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2015.01369/abstract},
	doi = {10.3389/fpsyg.2015.01369},
	abstract = {Although acoustic frequency is not a spatial property of physical objects, in common language, pitch, i.e., the psychological correlated of frequency, is often labeled spatially (i.e., “high in pitch” or “low in pitch”). Pitch-height is known to modulate (and interact with) the response of participants when they are asked to judge spatial properties of non-auditory stimuli (e.g., visual) in a variety of behavioral tasks. In the current study we investigated whether the modulatory action of pitch-height extended to the haptic estimation of height of a virtual step.We implemented a HW/SW setup which is able to render virtual 3D objects (stair-steps) haptically through a PHANTOM device, and to provide real-time continuous auditory feedback depending on the user interaction with the object. The haptic exploration was associated with a sinusoidal tone whose pitch varied as a function of the interaction point’s height within (i) a narrower and (ii) a wider pitch range, or (iii) a random pitch variation acting as a control audio condition. Explorations were also performed with no sound (haptic only). Participants were instructed to explore the virtual step freely, and to communicate height estimation by opening their thumb and index finger to mimic the step riser height, or verbally by reporting the height in centimeters of the step riser. We analyzed the role of musical expertise by dividing participants into non musicians and musicians. Results showed no effects of musical pitch on high-realistic haptic feedback. Overall there is no difference between the two groups in the proposed multimodal conditions. Additionally, we observed a different haptic response distribution between musicians and non musicians when estimations of the auditory conditions are matched with estimations in the no sound condition.},
	urldate = {2015-08-31},
	journal = {Front. Psychol.},
	author = {Geronazzo, Michele and Avanzini, Federico and Grassi, Massimo},
	year = {2015},
	keywords = {auditory pitch, haptic virtual objects, height estimation, multimodal virtual environment, sensory integration},
	pages = {1--11},
}

@article{spagnol_synthetic_2014,
	title = {Synthetic individual binaural audio delivery by pinna image processing},
	volume = {10},
	issn = {1742-7371},
	doi = {10.1108/IJPCC-06-2014-0035},
	number = {3},
	journal = {Int. J. of Pervasive Computing and Communications},
	author = {Spagnol, Simone and Geronazzo, Michele and Rocchesso, Davide and Avanzini, Federico},
	year = {2014},
	pages = {239--254},
}

@article{spagnol_relation_2013,
	title = {On the relation between pinna reflection patterns and head-related transfer function features},
	volume = {21},
	issn = {2329-9290},
	doi = {10.1109/TASL.2012.2227730},
	abstract = {This paper studies the relationship between head-related transfer functions (HRTFs) and pinna reflection patterns in the frontal hemispace. A pre-processed database of HRTFs allows extraction of up to three spectral notches from each response taken in the median sagittal plane. Ray-tracing analysis performed on the obtained notches’ central frequencies is compared with a set of possible reflection surfaces directly recognizeable from the corresponding pinna picture. Results of such analysis are discussed in terms of the reflection coefficient sign, which is found to be most likely negative. Based on this finding, a model for real-time HRTF synthesis that allows to control separately the evolution of different acoustic phenomena such as head diffraction, ear resonances, and reflections is proposed through the design of distinct filter blocks. Parameters to be fed to the model are derived either from analysis or from specific anthropometric features of the subject. Finally, objective evaluations of reconstructed HRTFs in the chosen spatial range are performed through spectral distortion measurements.},
	number = {3},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Spagnol, Simone and Geronazzo, Michele and Avanzini, Federico},
	month = mar,
	year = {2013},
	keywords = {HRTF, anthropometry, auditory displays, ear, my papers, pinna, spatial hearing},
	pages = {508--519},
}

@inproceedings{bahadori_action_2019,
	address = {Ferrara},
	title = {Action anticipation and sounds’ semantics},
	booktitle = {In {Proc}. of {XXVII} {SIPF} {National} {Congress}},
	publisher = {Società Italiana di Psicofisiologia e Neuroscienze Cognitive},
	author = {Bahadori, M. and Barumerli, R. and Geronazzo, M. and Cecco, R. and Passarin, C. and Marchioni, D. and Carner, M. and Cesari, P.},
	month = nov,
	year = {2019},
}

@article{geronazzo_external_2015,
	title = {The external ear acoustics: a mixed structural modeling approach in virtual auditory displays},
	volume = {39},
	issn = {2385-2615},
	language = {It},
	number = {1},
	journal = {J. of the Italian Society of Acoustics (RIA)},
	author = {Geronazzo, Michele},
	year = {2015},
	pages = {32--48},
}

@misc{barumerli_sofamyroom_2021,
	title = {{SofaMyRoom}: a fast and multiplatform "shoebox" room simulator for binaural room impulse response dataset generation},
	shorttitle = {{SofaMyRoom}},
	url = {http://arxiv.org/abs/2106.12992},
	abstract = {This paper introduces a shoebox room simulator able to systematically generate synthetic datasets of binaural room impulse responses (BRIRs) given an arbitrary set of head-related transfer functions (HRTFs). The evaluation of machine hearing algorithms frequently requires BRIR datasets in order to simulate the acoustics of any environment. However, currently available solutions typically consider only HRTFs measured on dummy heads, which poorly characterize the high variability in spatial sound perception. Our solution allows to integrate a room impulse response (RIR) simulator with different HRTF sets represented in Spatially Oriented Format for Acoustics (SOFA). The source code and the compiled binaries for different operating systems allow to both advanced and non-expert users to benefit from our toolbox, see https://github.com/spatialaudiotools/sofamyroom/ .},
	urldate = {2021-07-01},
	author = {Barumerli, Roberto and Bianchi, Daniele and Geronazzo, Michele and Avanzini, Federico},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.12992},
	keywords = {C.3, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, I.6.3, machine learning},
}

@incollection{barumerli_spatialized_2026,
	address = {Cham},
	title = {Spatialized {Looming} {Sounds} in {Virtual} {Reality}: {Reaction} {Times} and {Localization} {Accuracy}},
	volume = {15737},
	isbn = {978-3-031-97762-6 978-3-031-97763-3},
	shorttitle = {Spatialized {Looming} {Sounds} in {Virtual} {Reality}},
	url = {https://link.springer.com/10.1007/978-3-031-97763-3_19},
	abstract = {The recent availability of consumer head-mounted displays (HMDs) for virtual reality (VR) oﬀers an immersive and aﬀordable platform to study sound perception and action in plausible settings. Traditional perception-action studies often rely on complex systems such as motion capture, while HMDs integrate dynamic sound delivery and realtime movement tracking, making them more accessible for research outside controlled laboratory environments. In this study, we build upon the experimental protocol from Geronazzo et al. (2023) to evaluate the feasibility of measuring user behavior with a consumer-grade HMD, rather than relying on surface electromyography (EMG) to capture muscular activity and precise motion caption equipment. We focused on reaction times, direction and distance localization performances, utilizing the body tracking capabilities from the HMD’s onboard sensors and controllers to showcase the potential for consumer-grade VR to enable accessible experimental research.},
	language = {en},
	urldate = {2025-09-02},
	booktitle = {Extended {Reality}},
	publisher = {Springer Nature Switzerland},
	author = {Barumerli, Roberto and Privitera, Alessandro Giuseppe and Fasolato, Andrea and Liu, Xiang and Scarfò, Giuseppe and Cesari, Paola and Geronazzo, Michele},
	year = {2026},
	doi = {10.1007/978-3-031-97763-3_19},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {254--265},
}

@book{geronazzo_sonic_2023-1,
	address = {Cham},
	edition = {1},
	series = {Human–{Computer} {Interaction} {Series}},
	title = {Sonic {Interactions} in {Virtual} {Environments}},
	isbn = {978-3-031-04020-7 978-3-031-04021-4},
	url = {https://link.springer.com/10.1007/978-3-031-04021-4},
	language = {en},
	urldate = {2022-10-17},
	publisher = {Springer International Publishing},
	editor = {Geronazzo, Michele and Serafin, Stefania},
	year = {2023},
	doi = {10.1007/978-3-031-04021-4},
}

@incollection{geronazzo_sonic_2023,
	series = {Human–{Computer} {Interaction} {Series}},
	title = {Sonic {Interactions} in {Virtual} {Environments}: the {Egocentric} {Audio} {Perspective} of the {Digital} {Twin}},
	isbn = {978-3-031-04020-7},
	booktitle = {Sonic {Interactions} in {Virtual} {Environments}},
	publisher = {Springer London},
	author = {Geronazzo, Michele and Serafin, Stefania},
	year = {2023},
	pages = {3--48},
}

@book{geronazzo_proceedings_2020,
	title = {Proceedings of the 2020 {IEEE} 5th {VR} {Workshop} on {Sonic} {Interactions} for {Virtual} {Environments} ({SIVE})},
	isbn = {978-1-72816-532-5},
	publisher = {IEEE Computer Society},
	author = {Geronazzo, Michele and Nordahl, Rolf and De Götzen, Amalia and Erkut, Cumhur and Serafin, Stefania and Avanzini, Federico and Nilsson, Niels Christian and Grani, Francesco},
	month = mar,
	year = {2020},
	keywords = {3D sound, Conferences, Games, Solid modeling, Virtual Reality, multimodal interaction, music, sonic interaction design, sonification, tangible interfaces, virtual environments},
}

@article{geronazzo_interactions_2019,
	title = {Interactions in {Mobile} {Sound} and {Music} {Computing}},
	volume = {2019},
	doi = {10.1155/2019/5601609},
	language = {en},
	number = {5601609},
	urldate = {2020-01-01},
	journal = {Wireless Communications and Mobile Computing},
	author = {Geronazzo, Michele and Avanzini, Federico and Fontana, Federico and Serafin, Stefania},
	month = dec,
	year = {2019},
	doi = {10.1155/2019/5601609},
	pages = {1--2},
}

@incollection{andreasen_what_2019,
	series = {Lecture {Notes} of the {Institute} for {Computer} {Sciences}, {Social} {Informatics} and {Telecommunications} {Engineering}},
	title = {What {Is} {It} {Like} to {Be} a {Virtual} {Bat}?},
	isbn = {978-3-030-06134-0},
	abstract = {Virtual Reality (VR) might give us a glimpse of what it feels to have a different from human shaped body and how to orientate ourselves in virtual environment (VE) with it. Bats’ wings structure has anatomical similarities to a human hand; yet would it be possible to achieve a compelling illusion of virtual body ownership (VBO) over bat’s avatar is questionable. Hence our main aim of research is to imitate bat’s sonar system and achieve embodiment of anatomically similar but morphologically different body – a body of a bat. Test results showed a possibility to achieve VBO illusion using bat’s avatar. VBO was significantly higher when steering through VE, as opposed to steering without a virtual body and exposing to involuntary movement through VE. With our research prototype, users will be able to navigate with echolocation system and fly through a virtual cave.},
	language = {en},
	booktitle = {Interactivity, {Game} {Creation}, {Design}, {Learning}, and {Innovation}},
	publisher = {Springer International Publishing},
	author = {Andreasen, Anastassia and Nilsson, Niels Christian and Zovnercuka, Jelizaveta and Geronazzo, Michele and Serafin, Stefania},
	year = {2019},
	keywords = {Echolocation, Virtual Body Ownership, Virtual Reality},
	pages = {532--537},
}

@book{geronazzo_proceedings_2018,
	title = {Proceedings of the 2018 {IEEE} 4th {VR} {Workshop} on {Sonic} {Interactions} for {Virtual} {Environments} ({SIVE})},
	isbn = {978-1-5386-5713-3},
	publisher = {IEEE Computer Society},
	author = {Geronazzo, Michele and De Götzen, Amalia and Erkut, Cumhur and Serafin, Stefania and Avanzini, Federico and Nilsson, Niels Christian and Grani, Francesco},
	month = mar,
	year = {2018},
	keywords = {3D sound, Conferences, Games, Solid modeling, Virtual Reality, multimodal interaction, music, sonic interaction design, sonification, tangible interfaces, virtual environments},
}

@incollection{geronazzo_immersive_2018,
	title = {Immersive auralization using headphones},
	url = {https://link.springer.com/referenceworkentry/10.1007/978-3-319-08234-9_257-1},
	abstract = {Earphones; Headphone; Headphone impulse response; Headphone transfer function; Headphones; Headset Headphones are electro-acoustic transducers able to convert two electric output channels into two...},
	language = {en},
	urldate = {2018-05-02},
	booktitle = {Encyclopedia of {Computer} {Graphics} and {Games}},
	publisher = {Springer, Cham},
	author = {Geronazzo, Michele},
	year = {2018},
	doi = {10.1007/978-3-319-08234-9_257-1},
	pages = {1--5},
}

@incollection{geronazzo_user_2018,
	title = {User acoustics with head-related transfer functions},
	url = {https://link.springer.com/referenceworkentry/10.1007/978-3-319-08234-9_258-1},
	abstract = {Binaural hearing; Binaural sound; Head-related impulse response; Head-related transfer function; HRIR; HRTF; ILD; ITD; Spatial hearing; User acoustics Head-related impulse responses (HRIRs) or...},
	language = {en},
	urldate = {2018-05-01},
	booktitle = {Encyclopedia of {Computer} {Graphics} and {Games}},
	publisher = {Springer, Cham},
	author = {Geronazzo, Michele},
	year = {2018},
	doi = {10.1007/978-3-319-08234-9_258-1},
	pages = {1--5},
}

@incollection{geronazzo_sound_2018,
	title = {Sound spatialization},
	url = {https://link.springer.com/referenceworkentry/10.1007/978-3-319-08234-9_250-1},
	abstract = {Auralization; Binuaral headphone reproduction; Loud-speaker reproduction; Room acoustics; Room response equalization; Sound spatialization; Spatial room acoustic; Spatial room impulse response; Virtual...},
	language = {en},
	urldate = {2018-05-20},
	booktitle = {Encyclopedia of {Computer} {Graphics} and {Games}},
	publisher = {Springer, Cham},
	author = {Geronazzo, Michele},
	year = {2018},
	doi = {10.1007/978-3-319-08234-9_250-1},
	pages = {1--6},
}

@book{serafin_proceedings_2017,
	title = {Proceedings of the 2017 {IEEE} 3rd {VR} {Workshop} on {Sonic} {Interactions} for {Virtual} {Environments} ({SIVE})},
	isbn = {978-1-5386-0459-5},
	publisher = {IEEE Computer Society},
	author = {Serafin, Stefania and Nordahl, Rolf and De Götzen, Amalia and Erkut, Cumhur and Geronazzo, Michele and Avanzini, Federico and Nilsson, Niels Christian and Grani, Francesco},
	month = mar,
	year = {2017},
	keywords = {3D sound, Conferences, Games, Solid modeling, Virtual Reality, multimodal interaction, music, sonic interaction design, sonification, tangible interfaces, virtual environments},
}

@book{geronazzo_proceedings_2015,
	title = {Proceedings of the 2015 {IEEE} 2nd {VR} {Workshop} on {Sonic} {Interactions} for {Virtual} {Environments} ({SIVE})},
	isbn = {978-1-4799-1969-7},
	publisher = {IEEE Computer Society},
	author = {Geronazzo, Michele and Avanzini, Federico and Serafin, Stefania and Nordahl, Rolf and De Götzen, Amalia and Erkut, Cumhur},
	year = {2015},
}

@book{geronazzo_proceedings_2014,
	title = {Proceedings of the {XX} {Colloquium} on {Music} {Informatics}},
	isbn = {978-88-903413-3-5},
	publisher = {DADI - Dip. Arti e Design Industriale, IUAV University of Venice},
	author = {Geronazzo, Michele and Spagnol, Simone},
	month = dec,
	year = {2014},
}

@inproceedings{serafin_ieee_2025,
	title = {{IEEE} 8th {VR} {Workshop} on {Sonic} {Interactions} for {Virtual} {Environments}},
	url = {https://ieeexplore.ieee.org/document/10972894},
	doi = {10.1109/VRW66409.2025.00127},
	abstract = {Sonic interaction design is defined as the study and exploitation of sound as one of the principal channels conveying information, meaning, and aesthetic/emotional qualities in interactive contexts. This field lies at the intersection of interaction design, and sound and music computing.In the virtual reality community, the focus of research on topics related to auditory feedback has been rather limited when compared, for example, to the focus placed on visual feedback or even on haptic feedback. However, in such communities as film production or product sound design it is well known that sound is a powerful way to communicate meaning and emotion to a scene or a product.SIVE 2025 is the 8th of a series of workshops, whose main goal is to increase awareness among the virtual reality community of the importance of sonic elements when designing virtual/augmented/mixed reality (XR hereafter) environments. Participants to the workshop will also discuss how research in other related fields such as film sound theory, product sound design, sound and music computing, game sound design, and accessibility can inform designers of XR environments. Moreover, the workshop will feature state-of-the-art research in the field of sound for XR environments.},
	urldate = {2025-04-28},
	booktitle = {2025 {IEEE} {Conference} on {Virtual} {Reality} and {3D} {User} {Interfaces} {Abstracts} and {Workshops} ({VRW})},
	author = {Serafin, Stefania and Adjorlu, Ali and Nordahl, Rolf and Fontana, Federico and Geronazzo, Michele and Bouchara, Tifanie and Berthaut, Florent and Michon, Romain and Hammershøj, Dorte and Picinali, Lorenzo},
	month = mar,
	year = {2025},
	keywords = {Conferences, Games, H.5.5 [Information interfaces and presentation (e.g., HCI)]: Miscellaneous—eXtended reality virtual reality augmented reality, H.5.5 [Information interfaces and presentation (e.g., HCI)]: Sound and Music Computing—3D audio auditory display, Haptic interfaces, Music, Production, Spatial audio, Three-dimensional displays, User interfaces, Virtual environments, Visualization},
	pages = {611--612},
}

@inproceedings{barumerli_measuring_2025,
	title = {Measuring {Motor} {Planning} {Using} an {Affordable} {Sound}-{Based} {Virtual} {Reality} {Setup} for {Accessible} {Perception}-{Action} {Studies}},
	url = {https://ieeexplore.ieee.org/document/10972747},
	doi = {10.1109/VRW66409.2025.00128},
	abstract = {The recent availability of consumer head-mounted displays (HMDs) for virtual reality (VR) provides an immersive and affordable platform to study sound perception and action outside laboratory settings. While traditional perception-action studies rely on complex systems like motion capture, HMDs integrate dynamic sound delivery and real-time movement tracking. In this study, we adapted the experimental protocol from Geronazzo et al. (2023) to test the feasibility of measuring user behaviour with a consumer-grade HMD. Instead of muscular activity measured from surface electromyography, we analysed reaction times using kinematic data from the HMD’s onboard sensors and controllers. Data from ten participants showed that spatial properties of looming sounds influence auditory perception-action loops, aligning with findings from the original study. Our results demonstrate that reaction time is a scalable and meaningful measure for studying space perception through sound, providing a possible alternative to muscular activity measurements and showcasing the potential of consumer-grade VR for accessible and robust experimental research despite current limitations.},
	urldate = {2025-04-28},
	booktitle = {2025 {IEEE} {Conference} on {Virtual} {Reality} and {3D} {User} {Interfaces} {Abstracts} and {Workshops} ({VRW})},
	author = {Barumerli, Roberto and Privitera, Alessandro Giuseppe and Carollo, Alessandro and Fontana, Giovanni and Patarini, Eros and Zaninelli, Alberto and Cesari, Paola and Geronazzo, Michele},
	month = mar,
	year = {2025},
	keywords = {Atmospheric measurements, Current measurement, Particle measurements, Resists, Sensors, Three-dimensional displays, Time measurement, Tracking, User interfaces, Virtual reality, auditory perception, consumer devices, looming sounds, portable setup, reaction time},
	pages = {613--618},
}

@inproceedings{daugintis_effects_2024,
	address = {Redmond, WA, USA},
	title = {Effects of binaural rendering personalisation and reverberation on speech-on-speech masking},
	abstract = {This study investigates the effect of head-related transfer function (HRTF) personalisation on understanding binaurally rendered target speech masked by interfering speakers in reverberant conditions. During a listening test, participants had to identify a correct colour-number combination from a virtual talker rendered in front of them while ignoring two interfering talkers positioned either in front or at the back. The sound was rendered with either an individual HRTF or one of two non-individual ones. These were selected for each participant as the best or the worstmatching from the same HRTF dataset, based on predictions of a computational auditory model for sound localisation. Two types of reverb from measured spatial room impulse responses (SRIRs) were applied to the speech: realistic dichotic reverberation decoded from 4th-order Ambisonic SRIRs or diotic reverb based on the omnidirectional Ambisonic channel IR as a baseline. Preliminary results show that realistic dichotic reverb improves speech perception when interfering speech is co-located with the target. No significant differences were observed across HRTF conditions on a group level, but individual HRTF-related performance differences exist, requiring further intra-subject analyses and data collection to characterise the individual results.

Link to paper: https://aes2.org/publications/elibrary-page/?id=22671},
	publisher = {AES},
	author = {Daugintis, Rapolas and Alary, Benoit and Geronazzo, Michele and Picinali, Lorenzo},
	month = aug,
	year = {2024},
}

@inproceedings{zanoni_emanuele_caso_2024,
	address = {Toarmina},
	title = {Un {Caso} di {Studio} {Relativo} al {Sovracampionamento} {Spaziale} di {Head}-{Related} {Transfer} {Function} {Attraverso} {Reti} {Neurali} {Informate} dalla {Fisica}},
	abstract = {La Realtà Virtuale (VR) e Aumentata (AR) stanno cambiando il nostro modo di interagire con il mondo digitale. In questo contesto, l'audio spazializzato gioca un ruolo cruciale per la completa immersione in un ambiente virtuale. Le reti neurali informate dalla fisica, Physics-Informed Neural Networks (PINN), possono essere impiegate per l'upsampling spaziale di Head-Related Transfer Function (HRTF), semplificandone drasticamente i metodi di acquisizione e garantendo l’accessibilità all’audio spazializzato.},
	language = {it},
	booktitle = {In {Atti} 50° {Convegno} {Nazionale} {Asoociazione} {Italiana} di {Acustica}},
	author = {Zanoni, Emanuele, Fei and Gulli, Andrea and Geronazzo, Michele},
	year = {2024},
	keywords = {Audio and Speech Processing (eess.AS), FOS: Electrical engineering, electronic engineering, information engineering},
}

@inproceedings{gulli_mobile_2024,
	address = {Torino, Italy},
	title = {A mobile game app for adaptive assessment of pitch discrimination in children with different hearing ability},
	isbn = {978-88-903413-7-3},
	abstract = {Recent advancements in audiological testing and rehabilitation prioritize utmost personalization and minimal stress in patients, especially the young ones. Hearing health assessments integrate innovative approaches, emphasizing ecological listening scenarios and patient engagement. Driven by these principles, a mobile application tailored for pitch discrimination in children has been designed. The app implements intuitive game mechanics on a captivating graphical interface and harnesses machine learning algorithms to adapt the sound pressure levels to individual comfort levels. It utilizes simple yet effective acoustic stimuli obtained from second-order digital resonators, ensuring a more ecological approach. The pitch discrimination threshold is obtained with adaptive psychometric techniques to guarantee reliable and faster measurements. Preliminary qualitative evaluations involving normal hearing and single-sided deaf with cochlear implant children yield promising outcomes. The resulting perceptual thresholds align with established literature, envisioning the app’s efficacy in delivering accurate assessments. The presented tool paves the way for the use of gameplay in young hearing-impaired individuals’ rehabilitation after treatment with cochlear implants.},
	language = {en},
	booktitle = {In {Proc}, of the {XXIV} {Colloquio} di {Informatica} {Musicale}},
	author = {Gulli, Andrea and Fontana, Federico and Jarvelainen, Hanna and Geronazzo, Michele},
	year = {2024},
	pages = {123--128},
}

@inproceedings{daugintis_classifying_2023,
	title = {Classifying {Non}-{Individual} {Head}-{Related} {Transfer} {Functions} with {A} {Computational} {Auditory} {Model}: {Calibration} {And} {Metrics}},
	shorttitle = {Classifying {Non}-{Individual} {Head}-{Related} {Transfer} {Functions} with {A} {Computational} {Auditory} {Model}},
	doi = {10.1109/ICASSP49357.2023.10095152},
	abstract = {This study explores the use of a multi-feature Bayesian auditory sound localisation model to classify non-individual head-related transfer functions (HRTFs). Based on predicted sound localisation performance, these are grouped into ‘good’ and ‘bad’, and the ‘best’/‘worst’ is selected from each category. Firstly, we present a greedy algorithm for automated individual calibration of the model based on the individual sound localisation data. We then discuss data analysis of predicted directional localisation errors and present an algorithm for categorising the HRTFs based on the localisation error distributions within a limited range of directions in front of the listener. Finally, we discuss the validity of the classification algorithm when using averaged instead of individual model parameters. This analysis of auditory modelling results aims to provide a perceptual foundation for automated HRTF personalisation techniques for an improved experience of binaural spatial audio technologies.},
	booktitle = {{ICASSP} 2023 - 2023 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Daugintis, Rapolas and Barumerli, Roberto and Picinali, Lorenzo and Geronazzo, Michele},
	month = jun,
	year = {2023},
	keywords = {Bayesian inference, Classification algorithms, Computational modeling, HRTF personalisation, Predictive models, Signal processing, Signal processing algorithms, Spatial audio, Transfer functions, auditory modelling, human sound localisation},
	pages = {1--5},
}

@inproceedings{gulli_active_2023,
	address = {Copenhagen, Denmark},
	title = {An active learning procedure for the interaural time difference discrimination threshold},
	booktitle = {Proc. of the 26th {Int}. {Conference} on {Digital} {Audio} {Effects} ({DAFx}-23)},
	author = {Gulli, Andrea and Fontana, Federico and Serafin, Stefania and Geronazzo, Michele},
	month = sep,
	year = {2023},
	pages = {273--280},
}

@inproceedings{privitera_effect_2023,
	address = {New York, NY, USA},
	series = {{CHItaly} '23},
	title = {On the {Effect} of {User} {Tracking} on {Perceived} {Source} {Positions} in {Mobile} {Audio} {Augmented} {Reality}},
	isbn = {9798400708060},
	url = {https://dl.acm.org/doi/10.1145/3605390.3605422},
	doi = {10.1145/3605390.3605422},
	abstract = {Mobile Audio Augmented Reality (AAR) allows users to live sonic experience where virtual sound objects are integrated seamlessly with the real-world acoustic space. This paper focuses on user perception of the virtual source position and the impact of different out-of-the-box user tracking and head orientation methods made available by iPad’s Augmented Reality (AR) camera tracking system and AirPods Pro. We propose an experimental procedure to rank tracking approaches according to perceived accuracy in positioning eye-level and ground-level virtual audio sources compared to real source references. To correctly provide a plausible AAR scenario, the proposed consumer electronic setup simulates a virtual sound source employing the scattering delay network (SDN) algorithm for calibrated dynamic auralisation and customized head-related transfer functions (HRTFs) for personalized user acoustics. The natural listening experience of real sound sources leverages AirPods’s active signal processing algorithms for headphone hear-through capabilities. The main result of this study lies in observing an accommodation effect by users interacting with different tracking approaches and sound source positions. In summary, participants tend to prefer more stable tracking solutions despite accuracy and wide head movement range.},
	urldate = {2023-09-20},
	booktitle = {Proceedings of the 15th {Biannual} {Conference} of the {Italian} {SIGCHI} {Chapter}},
	publisher = {Association for Computing Machinery},
	author = {Privitera, Alessandro Giuseppe and Fontana, Federico and Geronazzo, Michele},
	month = sep,
	year = {2023},
	keywords = {Augumented reality, Earables., Sonic interactions in virtual environments, Spatial audio, User tracking, Virtual Acoustics},
	pages = {1--9},
}

@inproceedings{daugintis_initial_2023,
	address = {Turin, Italy},
	title = {Initial {Evaluation} of an {Auditory}-{Model}-{Aided} {Selection} {Procedure} for {Non}- {Individual} {HRTFs}},
	isbn = {978-88-88942-67-4},
	url = {https://dael.euracoustics.org/confs/landing_pages/fa2023/000489.html},
	doi = {10.61782/fa.2023.0489},
	abstract = {Binaural spatial audio reproduction systems use measured or simulated head-related transfer functions (HRTFs), which encode the effects of the outer ear and body on the incoming sound to recreate a realistic spatial auditory field around the listener. The sound localisation cues embedded in the HRTF are highly personal. Establishing perceptual similarity between different HRTFs in a reliable manner is challenging due to a combination of acoustic and non-acoustic aspects affecting our spatial auditory perception. To account for these factors, we propose an automated procedure to select the ‘best’ non-individual HRTF dataset from a pool of measured ones. For a group of human participants with their own acoustically measured HRTFs, a multi-feature Bayesian auditory sound localisation model is used to predict individual localisation performance with the other HRTFs from within the group. Then, the model selection of the ‘best’ and the ‘worst’ non-individual HRTFs is evaluated via an actual localisation test and a subjective audio quality assessment in comparison with individual HRTFs. A successful model-aided objective selection of the ‘best’ non-individual HRTF may provide relevant insights for effective and handy binaural spatial audio solutions in virtual/augmented reality (VR/AR) applications.},
	language = {en},
	urldate = {2024-01-24},
	booktitle = {Proceedings of the 10th {Convention} of the {European} {Acoustics} {Association} {Forum} {Acusticum} 2023},
	publisher = {European Acoustics Association},
	author = {Daugintis, R. and Barumerli, R. and Geronazzo, M. and Picinali, L.},
	month = sep,
	year = {2023},
	pages = {2701--2708},
}

@inproceedings{privitera_personalization_2023,
	address = {Turin, Italy},
	title = {Personalization in {Audio} {Storytelling} within {Virtual} and {Augmented} {Reality}: {State} of the {Art} and {Insights}},
	isbn = {978-88-88942-67-4},
	shorttitle = {Personalization in {Audio} {Storytelling} within {Virtual} and {Augmented} {Reality}},
	url = {https://dael.euracoustics.org/confs/landing_pages/fa2023/000430.html},
	doi = {10.61782/fa.2023.0430},
	abstract = {Storytelling and narrative are essential components in diverse contexts ranging from entertainment to culture and knowledge. With the progressive evolution of communication, stories also started to be transmitted through digital media. In particular, the emerging and promising field of sonic interactions in virtual environments (SIVE) fosters the creation of immersive experiences in which personalization of user interaction and narrative content can help create engaging and interactive storytelling. The insights presented here aim to define a state of the art of available interactive audio technologies for storytelling and to illustrate the role of audio in augmented and virtual reality (AR/VR) personalized experiences by reviewing a selection of published works. Moreover, we will try to disclose the most important elements enabling an immersive experience and to explain how interactivity, emotional and personalized content help convey messages in the context of serious storytelling. Finally, we will investigate the limitations of the available methods by also outlining some promising research directions.},
	language = {en},
	urldate = {2024-01-24},
	booktitle = {Proceedings of the 10th {Convention} of the {European} {Acoustics} {Association} {Forum} {Acusticum} 2023},
	publisher = {European Acoustics Association},
	author = {Privitera, A.G. and Fontana, F. and Geronazzo, M.},
	month = sep,
	year = {2023},
	pages = {2717--2720},
}

@inproceedings{privitera_preliminary_2023,
	address = {Turin, Italy},
	title = {Preliminary {Evaluation} of the {Auralization} of a {Real} {Indoor} {Environment} for {Augmented} {Reality} {Research}},
	isbn = {978-88-88942-67-4},
	url = {https://dael.euracoustics.org/confs/landing_pages/fa2023/000429.html},
	doi = {10.61782/fa.2023.0429},
	abstract = {This paper describes the calibration procedure and the technical setup for a realistic real-time acoustic reconstruction of a real indoor environment, a corridor, in the context of Audio Augmented Reality (AAR). The acoustic phenomena inside such space are simulated using the scattering delay network (SDN) algorithm. Wall reflection coefficients have been estimated using room dimensions, wall materials, and RT60 decay measurements. Auralisation has been dynamically conveyed by using a personalized head-related transfer function (HRTF), modeled by combining (i) a spherical head model with ear displacement with (ii) the high-frequency magnitude of an HRTF selected from the CIPIC database by using two 2D images of the user’s head. Moreover, the iPad’s AR camera tracking system and AirPods pro accelerometers have tracked the listener’s head and body position in real space. The proposed preliminary evaluation focuses on the impact of the different rendering factors in a simple AAR environment, suggesting that personalization, room calibration, and volume gain help render a more plausible AAR scene.},
	language = {en},
	urldate = {2024-01-24},
	booktitle = {Proceedings of the 10th {Convention} of the {European} {Acoustics} {Association} {Forum} {Acusticum} 2023},
	publisher = {European Acoustics Association},
	author = {Privitera, A.G. and Noro, M. and Geronazzo, M.},
	month = sep,
	year = {2023},
	pages = {4759--4766},
}

@inproceedings{okuno_effect_2023,
	address = {Nanjing, China},
	title = {Effect of {Auditory} {Stimulation} to {Task} and {Presence} in {Selective} {Blurred} {Immersive} {Environment} for {VR} {Sickness} {Reduction}},
	booktitle = {Proc. of {IEEJ} {International} {Workshop} on {Sensing}, {Actuation}, and {Motion} {Control}},
	publisher = {IEEJ},
	author = {Okuno, Satoshi and Shimizu, Sota and Privitera, Alessandro Giuseppe and Oboe, Roberto and Geronazzo, Michele},
	month = mar,
	year = {2023},
}

@inproceedings{geronazzo_egocentric_2022-1,
	address = {Verona},
	title = {The {Egocentric} {Audio} {Perspective} in {Virtual} {Environments}},
	isbn = {978-88-88942-63-6},
	booktitle = {Proc. of the 2nd {Symposium}: {The} {Acoustics} of {Ancient} {Theatres}},
	author = {Geronazzo, Michele},
	month = jul,
	year = {2022},
}

@inproceedings{geronazzo_egocentric_2022,
	title = {Egocentric {Audio} in the {Digital} {Twin} of {Virtual} {Environments}},
	doi = {10.1109/ICIR55739.2022.00017},
	abstract = {In Virtual Environments (VE), audio technologies play a significant role in immersive and interactive experiences. Virtual Reality (VR) simulations must be ecologically enacted by a participatory exploration of sense-making in a network of human and non-human agents, called actors. The guardian of such locus of agency is the digital twin (DT) that fosters intra-actions between humans and technology, dynamically and fluidly redefining all those configurations that are crucial for meaningful sonic experiences. The idea of human-machine entanglement is here mainly declined in an egocentric-spatial perspective related to emerging knowledge of the listener’s subjectivity. Such a systemic view can be interpreted as a working definition of intelligent reality: a perceptual and cognitive co-constitution of physical and virtual worlds through adaptive and reflective behaviors of VR technologies. The main theoretical results reported in this paper reside in the definition of sonic experiences as a multilayer interconnected network of actors lying in two main layers, i.e., immersion and coherence, which are entangled by a DT able to perform transformative actions for the listener.},
	booktitle = {2022 {IEEE} 2nd {International} {Conference} on {Intelligent} {Reality} ({ICIR})},
	author = {Geronazzo, Michele},
	month = dec,
	year = {2022},
	keywords = {Adaptive systems, Biological system modeling, Coherence, Digital twins, Nonhomogeneous media, Solid modeling, Virtual environments, digital twin, egocentric audio, immersive audio, intelligent reality, multi agent system, sonic experience, sonic interaction design, virtual reality},
	pages = {7--10},
}

@inproceedings{boren_comparison_2021,
	title = {Comparison of {Distortion} {Products} in {Headphone} {Equalization} {Algorithms} for {Binaural} {Synthesis}},
	url = {https://www.aes.org/e-lib/browse.cfm?elib=21094},
	abstract = {Headphone design has traditionally focused on creating a frequency response to make commercial stereo audio sound more natural. However, because of the sensitivity of spatial hearing to frequency-dependent cues, binaural reproduction requires headphones' target spectrum to be as flat as possible. Initial attempts to equalize headphones used a naive inversion of the headphone spectrum, which degraded binaural content because the headphone transfer function (HpTF) changes each time headphones...},
	language = {English},
	urldate = {2021-05-28},
	booktitle = {In {Proc}. of the {Audio} {Engineering} {Society} {Convention} 150},
	publisher = {Audio Engineering Society},
	author = {Boren, Braxton and Geronazzo, Michele},
	month = may,
	year = {2021},
}

@inproceedings{geronazzo_minimal_2020,
	address = {Barcelona, Spain},
	title = {A {Minimal} {Personalization} of {Dynamic} {Binaural} {Synthesis} with {Mixed} {Structural} {Modeling} and {Scattering} {Delay} {Networks}},
	isbn = {978-1-5090-6631-5},
	doi = {ICASSP40776.2020.9053873},
	abstract = {This paper provides a small set of essential parameters for a per-sonalized and effective real-time auralization with headphones. An image-guided procedure with two 2D images of the user's head guides the mixed structural modeling of head-related transfer function (HRTF), combining a spherical head model with ear displacement with the HRTF high-frequency magnitude selected from a database according to ear anthropometry. Room acoustics phenomena are simplified following the scattering delay network (SDN) approach which allows an accurate spatialization of first order reflections. Finally, statically significant improvements in localization performances within a virtual reality (VR) test allow to identify some benefits of the proposed customized auralization model compared to the widely used higher-order ambisonics (HOA) rendering with generic HRTFs.},
	booktitle = {Proc. {IEEE} {Int}. {Conf}. on {Acoust}. {Speech} {Signal} {Process}. ({ICASSP} 2020)},
	author = {Geronazzo, Michele and Tissieres, Jason Yves and Serafin, Stefania},
	month = may,
	year = {2020},
	pages = {411--415},
}

@inproceedings{barumerli_predicting_2020,
	title = {Predicting {Directional} {Sound}-{Localization} of {Human} {Listeners} in both {Horizontal} and {Vertical} {Dimensions}},
	url = {https://www.aes.org/e-lib/browse.cfm?elib=20777},
	abstract = {Measuring and understanding spatial hearing is a fundamental step to create effective virtual auditory displays (VADs). The evaluation of such auralization systems often requires psychoacoustic experiments. This process can be time consuming and error prone, resulting in a bottleneck for the evaluation complexity. In this work we evaluated a probabilistic auditory model for sound localization intended as a tool to assess VAD's abilities to provide static sound-localization cues to listeners....},
	language = {English},
	urldate = {2021-04-10},
	publisher = {Audio Engineering Society},
	author = {Barumerli, Roberto and Majdak, Piotr and Reijniers, Jonas and Baumgartner, Robert and Geronazzo, Michele and Avanzini, Federico},
	month = may,
	year = {2020},
}

@inproceedings{bahadori_reacting_2020,
	title = {Reacting to emotional sounds entering peripersonal space},
	booktitle = {In {Proc}. of the 4th {HBP} {Student} {Conf}. on {Interdisciplinary} {Brain} {Research} (accepted)},
	author = {Bahadori, M. and Barumerli, R. and Geronazzo, M. and Cecco, R. and Passarin, C. and Marchioni, D. and Carner, M. and Cesari, P.},
	year = {2020},
}

@inproceedings{geronazzo_impact_2018,
	address = {Munich, Germany},
	title = {The impact of an accurate vertical localization with {HRTFs} on short explorations of immersive virtual reality scenarios},
	doi = {10.1109/ISMAR.2018.00034},
	booktitle = {Proc. 17th {IEEE}/{ACM} {Int}. {Symposium} on {Mixed} and {Augmented} {Reality} ({ISMAR})},
	publisher = {IEEE Computer Society},
	author = {Geronazzo, Michele and Sikström, Erik and Kleimola, Jari and Avanzini, Federico and De Götzen, Amalia and Serafin, Stefania},
	month = oct,
	year = {2018},
	selected = {true},
	pages = {90--97},
}

@inproceedings{barumerli_localization_2018,
	title = {Localization in {Elevation} with {Non}-{Individual} {Head}-{Related} {Transfer} {Functions}: {Comparing} {Predictions} of {Two} {Auditory} {Models}},
	shorttitle = {Localization in {Elevation} with {Non}-{Individual} {Head}-{Related} {Transfer} {Functions}},
	doi = {10.23919/EUSIPCO.2018.8553320},
	abstract = {This paper explores the limits of human localization of sound sources when listening with non-individual Head-Related Transfer Functions (HRTFs), by simulating performances of a localization task in the mid-sagittal plane. Computational simulations are performed with the CIPIC HRTF database using two different auditory models which mimic human hearing processing from a functional point of view. Our methodology investigates the opportunity of using virtual experiments instead of time- and resource- demanding psychoacoustic tests, which could also lead to potentially unreliable results. Four different perceptual metrics were implemented in order to identify relevant differences between auditory models in a selection problem of best-available non-individual HRTFs. Results report a high correlation between the two models denoting an overall similar trend, however, we discuss discrepancies in the predictions which should be carefully considered for the applicability of our methodology to the HRTF selection problem.},
	booktitle = {2018 26th {European} {Signal} {Processing} {Conference} ({EUSIPCO})},
	author = {Barumerli, Roberto and Geronazzo, Michele and Avanzini, Federico},
	month = sep,
	year = {2018},
	note = {ISSN: 2076-1465},
	keywords = {Acoustics, Computational modeling, Databases, Ear, Measurement, Predictive models, torso},
	pages = {2539--2543},
}

@inproceedings{sikstrom_virtual_2018,
	address = {Cyprus},
	title = {Virtual reality exploration with different head-related transfer functions},
	booktitle = {Proc. 15th {Int}. {Conf}. {Sound} and {Music} {Computing} ({SMC} 2018)},
	author = {Sikström, Erik and Geronazzo, Michele and Kleimola, Jari and Avanzini, Federico and De Götzen, Amalia and Serafin, Stefania},
	month = jul,
	year = {2018},
	pages = {85--92},
}

@inproceedings{andreasen_navigate_2018,
	address = {Cyprus},
	title = {Navigate as a bat. real-time echolocation system in virtual reality},
	booktitle = {Proc. 15th {Int}. {Conf}. {Sound} and {Music} {Computing} ({SMC} 2018)},
	author = {Andreasen, Anastassia and Zovnercuka, Jelizaveta and Konovalovs, Kristian and Geronazzo, Michele and Paisa, Razvan and Serafin, Stefania},
	month = jul,
	year = {2018},
	pages = {198--205},
}

@inproceedings{barumerli_round_2018,
	address = {Reutlingen, Germany},
	title = {Round robin comparison of inter-laboratory {HRTF} measurements – assessment with an auditory model for elevation},
	isbn = {978-1-5386-5713-3},
	doi = {10.1109/SIVE.2018.8577091},
	booktitle = {Proc. of {IEEE} 4th {VR} {Workshop} on {Sonic} {Interactions} for {Virtual} {Environments} ({SIVE18})},
	publisher = {IEEE Computer Society},
	author = {Barumerli, Roberto and Geronazzo, Michele and Avanzini, Federico},
	month = mar,
	year = {2018},
	keywords = {club fritz},
}

@inproceedings{geronazzo_educational_2018,
	address = {Reutlingen, Germany},
	title = {An educational experience with motor planning and sound semantics in virtual audio reality},
	isbn = {978-1-5386-5713-3},
	doi = {10.1109/SIVE.2018.8577104},
	booktitle = {Proc. of {IEEE} 4th {VR} {Workshop} on {Sonic} {Interactions} for {Virtual} {Environments} ({SIVE18})},
	publisher = {IEEE Computer Society},
	author = {Geronazzo, Michele and Nardello, Francesca and Cesari, Paola},
	month = mar,
	year = {2018},
}

@inproceedings{geronazzo_improving_2017,
	address = {Edinburgh, UK},
	title = {Improving elevation perception with a tool for image-guided head-related transfer function selection},
	isbn = {ISSN: 2413-6700},
	booktitle = {Proc. of the 20th {Int}. {Conference} on {Digital} {Audio} {Effects} ({DAFx}-17)},
	author = {Geronazzo, Michele and Peruch, Enrico and Prandoni, Fabio and Avanzini, Federico},
	month = sep,
	year = {2017},
	keywords = {my papers, pinna, structural model},
	pages = {397--404},
}

@inproceedings{geronazzo_motion_2016,
	address = {Munich, Germany},
	title = {A motion based setup for peri-personal space estimation with virtual auditory displays},
	isbn = {978-1-4503-4491-3},
	doi = {10.1145/2993369.2996303},
	booktitle = {Proc. 22nd {ACM} {Symposium} on {Virtual} {Reality} {Software} and {Technology} ({VRST} 2016)},
	publisher = {ACM},
	author = {Geronazzo, Michele and Cesari, Paola},
	month = nov,
	year = {2016},
	pages = {299--300},
}

@inproceedings{geronazzo_acoustic_2016,
	address = {Munich, Germany},
	title = {Acoustic selfies for extraction of external ear features in mobile audio augmented reality},
	isbn = {978-1-4503-4491-3},
	doi = {10.1145/2993369.2993376},
	booktitle = {Proc. 22nd {ACM} {Symposium} on {Virtual} {Reality} {Software} and {Technology} ({VRST} 2016)},
	publisher = {ACM},
	author = {Geronazzo, Michele and Fantin, Jacopo and Sorato, Giacomo and Baldovino, Guido and Avanzini, Federico},
	month = nov,
	year = {2016},
	pages = {23--26},
}

@inproceedings{geronazzo_selfear_2016,
	address = {Hamburg, Germany},
	title = {The selfear project: a mobile application for low-cost pinna-related transfer function acquisition},
	isbn = {978-3-00-053700-4},
	booktitle = {Proc. 13th {Int}. {Conf}. {Sound} and {Music} {Computing} ({SMC} 2016)},
	author = {Geronazzo, Michele and Fantin, Jacopo and Sorato, Giacomo and Baldovino, Guido and Avanzini, Federico},
	month = sep,
	year = {2016},
	pages = {164--171},
}

@inproceedings{jeon_report_2015,
	address = {Nottingham},
	title = {Report on the in-vehicle auditory interactions workshop: taxonomy, challenges, and approaches.},
	url = {http://www.auto-ui.org/15/p/workshops/2/4_Report%20on%20the%20In-vehicle%20Auditory%20Interactions%20Workshop-%20Taxonomy,%20Challenges,%20and%20Approaches_Jeon.pdf},
	urldate = {2016-06-08},
	booktitle = {Proc. of the 7th {Int}. {Conf}. on {Automotive} {User} {Interfaces} and {Interactive} {Vehicular} {Applications} ({AutoUI} 2015)},
	publisher = {ACM},
	author = {Jeon, M. and et al.},
	month = sep,
	year = {2015},
	pages = {1--5},
}

@inproceedings{geronazzo_use_2015,
	address = {Graz, Austria},
	title = {Use of personalized binaural audio and interactive distance cues in an auditory goal-reaching task},
	isbn = {978-3-902949-01-1},
	booktitle = {Proc. of the 21st  {Int}. {Conf}. on {Auditory} {Display} ({ICAD} 2015)},
	author = {Geronazzo, Michele and Avanzini, Federico and Fontana, Federico},
	month = jul,
	year = {2015},
	pages = {73--80},
}

@inproceedings{boren_coloration_2015,
	address = {Graz, Austria},
	title = {Coloration metrics for headphone equalization},
	isbn = {978-3-902949-01-1},
	booktitle = {Proc. of the 21st  {Int}. {Conf}. on {Auditory} {Display} ({ICAD} 2015)},
	author = {Boren, Braxton and Geronazzo, Michele and Brinkmann, Fabian and Choueiri, Edgar},
	month = jul,
	year = {2015},
	pages = {29--34},
}

@inproceedings{geronazzo_evaluating_2015,
	address = {Arles, France},
	title = {Evaluating vertical localization performance of {3D} sound rendering models with a perceptual metric},
	doi = {10.1109/SIVE.2015.7361293},
	abstract = {The head-related transfer functions (HRTFs) describe individual acoustic transformation that sound sources undergo due to human anatomy before arriving at the left and right tympanic membranes. The resulting spectral modifications are the main localization cues for elevation detection in space. In this paper, synthetic HRTF mod- els able to render the vertical spatial dimension in virtual auditory displays, are evaluated via auditory models. Perceptually-motivated metrics describe the output of 4 virtual experiments that numeri- cally simulate real listening experiments for 20 virtual subjects. The current implementation considers a limited set of parameters for a structural model of the pinna acting as a proof-of-concept of such approach. Accordingly, results confirm that the research framework is a flexible tool for systematic evaluation of different instances of structural model.},
	booktitle = {2015 {IEEE} 2nd {VR} {Workshop} on {Sonic} {Interactions} for {Virtual} {Environments} ({SIVE})},
	publisher = {IEEE Computer Society},
	author = {Geronazzo, M. and Carraro, A. and Avanzini, F.},
	month = mar,
	year = {2015},
	keywords = {Acoustics, Computational modeling, Gain, Indexes, Measurement, Transfer functions, ear},
	pages = {1--5},
}

@inproceedings{geronazzo_personalization_2015,
	address = {Paris, France},
	series = {{WAC} '15},
	title = {Personalization support for binaural headphone reproduction in web browsers},
	isbn = {2663-5844},
	booktitle = {Proc. 1st {Web} {Audio} {Conference}},
	publisher = {IRCAM},
	author = {Geronazzo, Michele and Kleimola, Jari and Majdak, Piotr},
	month = jan,
	year = {2015},
	keywords = {web audio},
}

@inproceedings{boren_phona:_2014,
	title = {Phona: a public dataset of measured headphone transfer functions},
	isbn = {978-1-63439-748-3},
	shorttitle = {Phona},
	url = {http://www.aes.org/e-lib/browse.cfm?elib=17449},
	abstract = {A dataset of measured headphone transfer functions (HpTFs), the Princeton Headphone Open Archive (PHOnA), is presented. Extensive studies of HpTFs have been conducted for the past twenty years, each requiring a separate set of measurements, but this data has not yet been publicly shared. PHOnA aggregates HpTFs from different laboratories, including measurements for multiple different headphones, subjects, and repositionings of headphones for each subject. The dataset uses the spatially...},
	language = {English},
	urldate = {2014-10-30},
	booktitle = {Proc. 137th {Conv}. {Audio} {Eng}. {Society}},
	publisher = {Audio Engineering Society},
	author = {Boren, Braxton B. and Geronazzo, Michele and Majdak, Piotr and Choueiri, Edgar},
	month = oct,
	year = {2014},
}

@inproceedings{geronazzo_multimodal_2014,
	address = {Helsinki, Finland},
	title = {Multimodal exploration of virtual objects with a spatialized anchor sound},
	isbn = {978-0-937803-99-8},
	abstract = {A multimodal interactive system for audio-haptic integration is presented in this paper. Preliminary subjective tests with a virtual reality setup were conducted with the goal of interpreting cognitive mechanisms and improving performances in orientation \& mobility protocols for visually impaired subjects, where spatial representations need to be developed using residual sensory channels. An object recognition experiment was performed in order to investigate the contribution of dynamic...},
	language = {English},
	urldate = {2014-09-15},
	booktitle = {Proc. 55th {Int}. {Conf}. {Audio} {Eng}. {Society}, {Spatial} {Audio}},
	author = {Geronazzo, Michele and Bedin, Alberto and Brayda, Luca and Avanzini, Federico},
	month = aug,
	year = {2014},
	pages = {1--8},
}

@inproceedings{geronazzo_enhancing_2014,
	address = {Florence, Italy},
	title = {Enhancing vertical localization with image-guided selection of non-individual head-related transfer functions},
	doi = {10.1109/ICASSP.2014.6854446},
	abstract = {A novel approach to the selection of generic head-related transfer functions (HRTFs) for binaural audio rendering through headphones is formalized and described in this paper. A reflection model applied to the user's ear picture allows to extract the relevant anthropometric cues that are used for selecting two HRTF sets in a database fitting that user, whose localization performances are evaluated in a complete psychoacoustic experiment. The proposed selection increases the average elevation performances of 17\% (with a peak of 34\%) with respect to generic HRTFs from an anthropomorphic mannequin. It also significantly enhances externalization and reduces the number of up/down reversals.},
	booktitle = {{IEEE} {Int}. {Conf}. on {Acoust}. {Speech} {Signal} {Process}. ({ICASSP} 2014)},
	author = {Geronazzo, Michele and Spagnol, Simone and Bedin, Alberto and Avanzini, Federico},
	month = may,
	year = {2014},
	keywords = {HRTF, HRTF selection, anthropometric ear parameters, binaural audio, elevation, my papers, pinna, spatial hearing},
	pages = {4496--4500},
}

@inproceedings{spagnol_extraction_2013,
	address = {Vienna, Austria},
	title = {Extraction of pinna features for customized binaural audio delivery on mobile devices},
	booktitle = {Proc. 11th {International} {Conference} on {Advances} in {Mobile} {Computing} \& {Multimedia} ({MoMM}’13)},
	author = {Spagnol, Simone and Geronazzo, Michele and Rocchesso, Davide and Avanzini, Federico},
	month = dec,
	year = {2013},
	keywords = {anthropometric ear parameters, mobile devices, my papers},
	pages = {514--517},
}

@inproceedings{geronazzo_influence_2013,
	address = {Marseille},
	title = {Influence of auditory pitch on haptic estimation of spatial height},
	isbn = {978-2-909669-23-6},
	abstract = {This paper presents an experiment aimed at assessing the
influence of auditory feedback on haptic estimation of size. Experimental
subjects were instructed to explore a virtual 3D object (a stair-step)
with a haptic device, and to return a verbal estimate of the step riser
height. Haptic exploration was accompanied with a real-time generated
sinusoid whose pitch varied as a function of the interaction point’s height
within two different ranges. Experimental results show that the haptic
estimation is robust and accurate regardless the frequency range of the
accompanying sound.},
	booktitle = {Proc. 10th {International} {Symposium} on {Computer} {Music} {Multidisciplinary} {Research} ({CMMR}'13)},
	author = {Geronazzo, Michele and Avanzini, Federico and Grassi, Massimo},
	year = {2013},
	keywords = {haptics, height, multimodal interaction, my papers, pitch},
	pages = {759--765},
}

@inproceedings{spagnol_automatic_2013,
	address = {Pula, Italy},
	title = {Automatic extraction of pinna edges for binaural audio customization},
	doi = {10.1109/MMSP.2013.6659305},
	abstract = {The contribution of the external ear to the head-related transfer function (HRTF) heavily depends on the listener’s unique anthropometry. In particular, the shape of the most prominent contours of the pinna defines the frequency location of the HRTF spectral notches along the elevation of the sound source. This paper addresses the issue of automatically estimating the location of pinna edges starting from a set of pictures produced by a multi-flash imaging device. A basic image processing algorithm designed to obtain the principal edges and their distance from the ear canal entrance is described. The effectiveness of the developed hardware and software is preliminarily evaluated on a small number of test subjects.},
	booktitle = {Proc. {IEEE} {Int}. {Work}. {Multi}. {Signal} {Process}. ({MMSP} 2013)},
	author = {Spagnol, Simone and Rocchesso, Davide and Geronazzo, Michele and Avanzini, Federico},
	month = oct,
	year = {2013},
	keywords = {anthropometric ear parameters, anthropometric measurements, arduino, ear, image processing, my papers, pinna},
	pages = {301--306},
}

@inproceedings{geronazzo_mixed_2013,
	address = {Santorini, Greece},
	title = {Mixed structural modeling of head-related transfer functions for customized binaural audio delivery},
	isbn = {978-1-4673-5805-7},
	doi = {10.1109/ICDSP.2013.6622764},
	abstract = {A novel approach to the modeling of head-related transfer functions (HRTFs) for binaural audio rendering is formalized and described in this paper. Mixed structural modeling (MSM) can be seen as the generalization and extension of the structural modeling approach first defined by Brown and Duda back in 1998. Possible solutions for building partial HRTFs (pHRTFs) of the head, torso, and pinna of a specific listener are first described and then used in the construction of two possible mixed structural models of a KEMAR mannequin. Thanks to the flexibility of the MSM approach, an exponential number of solutions for building custom binaural audio displays can be considered and evaluated, the final aim of the process being the achievement of a HRTF model fully customizable by the listener.},
	booktitle = {Proc. 18th {Int}. {Conf}. {Digital} {Signal} {Process}. ({DSP} 2013)},
	author = {Geronazzo, Michele and Spagnol, Simone and Avanzini, Federico},
	month = jul,
	year = {2013},
	keywords = {HRTF, mixed structural model, my papers, structural decomposition, structural model},
	pages = {1--8},
}

@inproceedings{geronazzo_standardized_2013,
	address = {Rome, Italy},
	title = {A standardized repository of head-related and headphone impulse response data},
	isbn = {978-1-62748-571-5},
	url = {http://www.aes.org/e-lib/browse.cfm?elib=16802},
	booktitle = {Proc. 134th {Conv}. {Audio} {Eng}. {Society}},
	author = {Geronazzo, Michele and Granza, Fabrizio and Spagnol, Simone and Avanzini, Federico},
	year = {2013},
	keywords = {HRTF database, HpIR database, headphone reproduction, headphones, my papers},
}

@inproceedings{geronazzo_modular_2013,
	address = {Rome, Italy},
	title = {A modular framework for the analysis and synthesis of head-related transfer functions},
	isbn = {978-1-62748-571-5},
	abstract = {The paper gives an overview of a number of tools for the analysis and synthesis of head-related transfer functions (HRTFs) that we have developed in the past four years at the Department of Information Engineering, University of Padova, Italy. The main objective of our study in this context is the progressive development of a collection of algorithms for the construction of a totally synthetic personal HRTF set replacing both cumbersome and tedious individual HRTF measurements and the exploitation of inaccurate non-individual HRTF sets. Our research methodology is highlighted, along with the multiple possibilities of present and future research offered by such tools.},
	booktitle = {Proc. 134th {Conv}. {Audio} {Eng}. {Society}},
	author = {Geronazzo, Michele and Spagnol, Simone and Avanzini, Federico},
	month = may,
	year = {2013},
	keywords = {HRTF, mixed structural model, my papers, structural model},
}

@inproceedings{spagnol_hearing_2012,
	address = {Bucharest, Romania},
	title = {Hearing distance: a low-cost model for near-field binaural effects},
	abstract = {An extremely low-order filter model for source distance rendering in binaural reproduction is proposed in this paper. The main purpose of such model is to cheaply simulate the effect that source-listener distance has on the sound waves arriving at the ears in the near field, a region where the relation between sound pressure and distance is both highly frequency-dependent and nonlinear. The reference for the model is based on an analytical description of a spherical head response, appropriately filtered out so as to include distance-dependent patterns only. To this regard, the model is objectively seen to provide an excellent fit in the whole near field, despite its simplicity.},
	booktitle = {Proc. {EUSIPCO} 2012 {Conf}.},
	author = {Spagnol, Simone and Geronazzo, Michele and Avanzini, Federico},
	month = sep,
	year = {2012},
	keywords = {HRTF, distance rendering, my papers, near-field, spatial sound},
	pages = {2005--2009},
}

@inproceedings{spagnol_employing_2012,
	address = {Copenhagen, Denmark},
	title = {Employing spatial sonification of target motion in tracking exercises},
	abstract = {This paper presents the results of an experiment in which the effect of spatial sonification of a moving target on the user’s performance during the execution of basic tracking exercises was investigated. Our starting hypothesis is that a properly designed multimodal continuous feedback could be used to represent temporal and spatial information that can in turn improve performance and motor learning of simple target following tasks. Sixteen subjects were asked to track the horizontal movement of a circular visual target by controlling an input device with their hand. Two different continuous task-related auditory feedback modalities were considered, both simulating the sound of a rolling ball, the only difference between them being the presence or absence of binaural spatialization of the target’s position. Results demonstrate how spatial auditory feedback significantly decreases the average tracking error with respect to visual feedback alone, contrarily to monophonic feedback. It was thus found how spatial information provided through sound in addition to visual feedback helps subjects improving their performance.},
	booktitle = {Proc. 9th {Int}. {Conf}. {Sound} and {Music} {Computing} ({SMC} 2012)},
	author = {Spagnol, Simone and Geronazzo, Michele and Avanzini, Federico and Oscari, Fabio and Rosati, Giulio},
	month = jul,
	year = {2012},
	keywords = {my papers, rehab, rolling},
	pages = {85--89},
}

@inproceedings{geronazzo_head-related_2011,
	address = {Dijon, France},
	title = {A head-related transfer function model for real-time customized 3-d sound rendering},
	isbn = {978-0-7695-4635-3},
	doi = {10.1109/SITIS.2011.21},
	abstract = {This paper addresses the problem of modeling head-related transfer functions (HRTFs) for 3-D audio rendering in the front hemisphere. Following a structural approach, we build a model for real-time HRTF synthesis which allows to control separately the evolution of different acoustic phenomena such as head diffraction, ear resonances, and reflections through the design of distinct filter blocks. Parameters to be fed to the model are both derived from mean spectral features in a collection of measured HRTFs and anthropometric features of the specific subject (taken from a photograph of his/her outer ear), hence allowing model customization. Visual analysis of the synthesized HRTFs reveals a convincing correspondence between original and reconstructed spectral features in the chosen spatial range. Furthermore, a possible experimental setup for dynamic psychoacoustical evaluation of such model is depicted.},
	booktitle = {Proc. 7th {Int}. {Conf}. on {Signal} {Image} {Technology} \& {Internet}-{Based} {Systems} ({SITIS} 2011)},
	author = {Geronazzo, Michele and Spagnol, Simone and Avanzini, Federico},
	month = dec,
	year = {2011},
	keywords = {anthropometric ear parameters, elevation, my papers, pinna},
	pages = {174--179},
}

@inproceedings{spagnol_fitting_2010,
	address = {Saint-Malo, France},
	title = {Fitting pinna-related transfer functions to anthropometry for binaural sound rendering},
	doi = {10.1109/MMSP.2010.5662018},
	abstract = {This paper faces the general problem of modeling pinna-related transfer functions (PRTFs) for 3-D sound rendering. Following a structural approach, we aim at constructing a model for PRTF synthesis which allows to control separately the evolution of ear resonances and spectral notches through the design of two distinct filter blocks. Taking such model as endpoint, we propose a method based on the McAulay-Quatieri partial tracking algorithm to extract the frequencies of the most important spectral notches. Ray-tracing analysis performed on the so obtained tracks reveals a convincing correspondence between extracted frequencies and pinna geometry of a bunch of subjects.},
	booktitle = {Proc. {IEEE} {Int}. {Work}. {Multi}. {Signal} {Process}. ({MMSP}'10)},
	author = {Spagnol, Simone and Geronazzo, Michele and Avanzini, Federico},
	month = oct,
	year = {2010},
	keywords = {anthropometric ear parameters, my papers, pinna},
	pages = {194--199},
}

@inproceedings{geronazzo_estimation_2010,
	address = {Graz, Austria},
	title = {Estimation and modeling of pinna-related transfer functions},
	isbn = {978-3-200-01940-9},
	booktitle = {Proc. of the 13th {Int}. {Conference} on {Digital} {Audio} {Effects} ({DAFx}-10)},
	author = {Geronazzo, Michele and Spagnol, Simone and Avanzini, Federico},
	month = sep,
	year = {2010},
	keywords = {my papers, pinna, structural model},
	pages = {431--438},
}

@inproceedings{spagnol_structural_2010-1,
	address = {Torino, Italy},
	title = {Structural modeling of pinna-related transfer functions for 3-d sound rendering},
	abstract = {This paper considers the general problem of modeling pinna-related transfer functions (PRTFs) for 3-D sound rendering. Following a structural approach, we present an algorithm for the decomposition of PRTFs into ear resonances and frequency notches due to reflections over pinna cavities and exploit it in order to deliver a method to extract the frequencies of the most important spectral notches. Ray-tracing analysis reveals a convincing correspondence between extracted frequencies and pinna cavities of a bunch of subjects. We then propose a model for PRTF synthesis which allows to control separately the evolution of resonances and spectral notches through the design of two distinct filter blocks. The resulting model is suitable for future integration into a structural head-related transfer function model, and for parametrization over anthropometrical measurements of a wide range of subjects.},
	booktitle = {Proc. {XVIII} {Colloquio} di {Informatica} {Musicale} ({XVIII} {CIM})},
	author = {Spagnol, Simone and Geronazzo, Michele and Avanzini, Federico},
	month = oct,
	year = {2010},
	keywords = {my papers, pinna, structural model},
	pages = {92--101},
}

@inproceedings{geronazzo_laudio_2022,
	address = {Ancona, Italy},
	title = {L’{Audio} {Egocentrico} negli {Ambienti} {Virtuali}},
	isbn = {978-88-903413-6-6},
	booktitle = {Proc. {XXIII} {Colloquio} di {Informatica} {Musicale} ({XXII} {CIM})},
	author = {Geronazzo, Michele},
	month = oct,
	year = {2022},
}

@inproceedings{geronazzo_tecnologie_2018,
	address = {Udine, Italy},
	title = {Tecnologie per l'interazione sonora in contesti di realtà virtuale e aumentata immersiva},
	booktitle = {Proc. {XXII} {Colloquio} di {Informatica} {Musicale} ({XXII} {CIM})},
	author = {Geronazzo, Michele},
	month = nov,
	year = {2018},
}

@inproceedings{geronazzo_tecnologie_2018-1,
	address = {Udine, Italy},
	title = {Tecnologie per la didattica musicale: un'esperienza con la realtà virtuale},
	booktitle = {Proc. {XXII} {Colloquio} di {Informatica} {Musicale} ({XXII} {CIM})},
	author = {Geronazzo, Michele and Degli Innocenti, Edoardo and Nordahl, Rolf and Serafin, Stefania and Avanzini, Federico},
	month = nov,
	year = {2018},
}

@inproceedings{geronazzo_selfie_2016,
	address = {Cagliari, Italy},
	title = {Selfie acustiche con il progetto selfear: un'applicazione mobile per l'acquisizione a basso costo di pinna-related transfer function},
	booktitle = {Proc. {XXI} {Colloquio} di {Informatica} {Musicale} ({XXI} {CIM})},
	author = {Geronazzo, Michele and Fantin, Jacopo and Sorato, Giacomo and Baldovino, Guido and Avanzini, Federico},
	month = sep,
	year = {2016},
	pages = {129--136},
}

@inproceedings{geronazzo_audio_2014,
	address = {Rome, Italy},
	title = {Audio 3d e ancoraggio sonoro per l'esplorazione multimodale di ambienti virtuali},
	booktitle = {Proc. {XX} {Colloquium} on {Musical} {Informatics} ({XX} {CIM} 2014)},
	author = {Geronazzo, Michele and Brayda, Luca and Bedin, Alberto and Avanzini, Federico},
	month = nov,
	year = {2014},
	pages = {107--112},
}

@inproceedings{scaiella_valutazione_2014,
	address = {Rome, Italy},
	title = {Valutazione parametrica di un modello strutturale di orecchio esterno per il rendering binaurale del suono},
	booktitle = {Proc. {XX} {Colloquium} on {Musical} {Informatics} ({XX} {CIM} 2014)},
	author = {Scaiella, Sandro and Spagnol, Simone and Geronazzo, Michele and Avanzini, Federico},
	month = nov,
	year = {2014},
	pages = {47--52},
}

@inproceedings{geronazzo_nuovo_2014,
	address = {Pisa, Italy},
	title = {Un nuovo approccio a modelli strutturali misti per la sintesi e la personalizzazione di hrtf},
	isbn = {978-88-88942-47-6},
	language = {It},
	booktitle = {Proc. 41st {Convegno} {Nazionale} {Associazione} {Italiana} di {Acustica} (41 {AIA} 2014)},
	author = {Geronazzo, Michele and Spagnol, Simone and Avanzini, Federico},
	month = jun,
	year = {2014},
	pages = {1--8},
}

@inproceedings{geronazzo_model-based_2012,
	address = {Trieste, Italy},
	title = {Model-based customized binaural reproduction through headphones},
	abstract = {Generalized head-related transfer functions (HRTFs) represent a cheap and straightforward mean of providing 3D rendering in headphone reproduction. However, they are known to produce evident sound localization errors, including incorrect perception of elevation, front-back reversals, and lack of externalization, especially when head tracking is not utilized in the reproduction . Therefore, individual anthropometric features have a key role in characterizing HRTFs. On the other hand, HRTF measurements on a significant number of subjects are both expensive and inconvenient. This short paper briefly presents a structural HRTF model that, if properly rendered through a proposed hardware (wireless headphones augmented with motion and vision sensors), can be used for an efficient and immersive sound reproduction. Special care is reserved to the contribution of the external ear to the HRTF: data and results collected to date by the authors allow parametrization of the model according to individual anthropometric data, which in turn can be automatically estimated through straightforward image analysis. The proposed hardware and software can be used to render scenes with multiple audiovisual objects in a number of contexts such as computer games, cinema, edutainment, and many others.},
	booktitle = {Proc. {XIX} {Colloquio} di {Informatica} {Musicale} ({XIX} {CIM})},
	author = {Geronazzo, Michele and Spagnol, Simone and Rocchesso, Davide and Avanzini, Federico},
	month = nov,
	year = {2012},
	keywords = {my papers, structural model},
	pages = {212--213},
}

@inproceedings{geronazzo_customized_2011,
	address = {Alghero, Italy},
	title = {Customized {3D} {Sound} for {Innovative} {Interaction} {Design}},
	abstract = {This paper considers the impact of binaural 3D audio on several kinds of applications, classified according to their degree of body immersion and their own coordinate system deviation from a physical condition. A model for sound spatialization, which includes additional features with respect to existing systems, is introduced. A significant reduction of computational costs is allowed by model parametrization according to anthropometric information of the user and audio processing through low-order filters, thus resulting affordable for several kinds of devices. According to the following examination, this approach to 3D sound rendering can grant a transversal enrichment to the CHI research purposes, in reference to content creation and adaptation, resourceful delivery and augmented media presentation. In several contexts where personalized spatial sound reproduction is a central requirement, the quality of the immersive experience could only benefit from this sort of adaptable and modular system.},
	booktitle = {Proc. {SMC}-{HCI} {Work}., {CHItaly} 2011 {Conf}.},
	author = {Geronazzo, Michele and Spagnol, Simone and Avanzini, Federico},
	month = sep,
	year = {2011},
	keywords = {Virtual Reality, anthropometric ear parameters, augmented reality, my papers, pinna},
}

@inproceedings{spagnol_structural_2010,
	address = {Barcelona, Spain},
	title = {Structural modeling of pinna-related transfer functions},
	abstract = {This paper faces the general problem of modeling pinna-related transfer functions (PRTFs) for 3-D sound rendering. Following a structural modus operandi, we exploit an algorithm for the decomposition of PRTFs into ear resonances and frequency notches due to reflections over pinna cavities in order to deliver a method to extract the frequencies of the most important spectral notches. Ray-tracing analysis reveals a convincing correspondence between extracted frequencies and pinna cavities of a bunch of subjects. We then propose a model for PRTF synthesis which allows to control separately the evolution of resonances and spectral notches through the design of two distinct filter blocks. The resulting model is suitable for future integration into a structural head-related transfer function model, and for parametrization over anthropometrical measurements of a wide range of subjects.},
	booktitle = {Proc. 7th {Int}. {Conf}. {Sound} and {Music} {Computing} ({SMC} 2010)},
	author = {Spagnol, Simone and Geronazzo, Michele and Avanzini, Federico},
	month = jul,
	year = {2010},
	keywords = {my papers, pinna, structural model},
	pages = {422--428},
}

@inproceedings{takase_visibility_2022,
	address = {Tokyo, Japan},
	title = {Visibility control based on approximate expression gaze rate by {GMM}},
	booktitle = {{IEEJ} {Advanced} sensor information processing technology and its application},
	author = {Takase, Miwa and Shimizu, Sota and Morimoto, Takumi and Okuno, Satoshi and Geronazzo, Michele and Oboe, Roberto},
	month = jan,
	year = {2022},
}

@inproceedings{okuno_effects_2022,
	address = {Tokyo, Japan},
	title = {Effects of {Auditory} {Stimulation} on {Tasks} and {Presence} in {Selective} {Blur} {Immersive} {Virtual} {Environments} to {Reduce} {VR} {Sickness}},
	booktitle = {{IEEJ} {Advanced} sensor information processing technology and its application},
	author = {Okuno, Satoshi and Shimizu, Sota and Iida, Kohei and Geronazzo, Michele and Privitera, Alessandro Giuseppe and Oboe, Roberto},
	month = jan,
	year = {2022},
}

@inproceedings{daugintis_development_2022,
	title = {Development and evaluation of auditory-model-aided non-individual {HRTF} selection procedure},
	booktitle = {{UK} {Hearing} {Audiology} and {Sciences} {Meeting}},
	author = {Daugintis, Rapolas and Geronazzo, Michele and Barumerli, Roberto and Picinali, Lorenzo},
	month = sep,
	year = {2022},
}

@inproceedings{barumerli_evaluation_2022,
	address = {Gyeongju},
	title = {Evaluation of spatial tasks in virtual acoustic environments by means of modeling individual localization performances},
	booktitle = {Proceedings: {A21}, {Virtual} {Acoustics}, {ICA} 2022},
	author = {Barumerli, R. and Majdak, P. and Geronazzo, M. and Avanzini, F. and Meijer, D. and Baumgartner, R.},
	year = {2022},
	pages = {64--66},
}

@misc{picinali_sonicom_2021,
	address = {Bologna},
	title = {The {SONICOM} project: {AI}-driven immersive audio, from personalisation to modelling},
	author = {Picinali, Lorenzo and Geronazzo, Michele and Goodman, Dan F. M. and Katz, B. F. G. and Majdak, Piotr and Avanzini, Federico and Andreopoulou, Areti and Reyes-Lecuona, Arcadio and Vinciarelli, Alessandro and Brewster, Stephen},
	month = oct,
	year = {2021},
}

@inproceedings{barumerli_modeling_2021,
	address = {Vienna, Austria},
	title = {Modeling {Sound} {Localization} within the {Framework} of {Bayesian} {Inference}},
	abstract = {In this work, we propose an auditory model based on Bayesian inference to reproduce the individual human ability to localize a sound source in the acoustic free field. The model combines physiologically motivated front-ends with a probabilistic decision stage in order to estimate both the lateral- and polar-angle components of the incoming sound direction. In our systematical evaluation, the model was able to reproduce the summary statistics from five listeners when localizing broad-band sound sources. In particular, the results indicate that the model required to account for both the acoustic interaction of the sound source with the subject anatomy and for non-acoustic factors as neural uncertainties and sensorimotor mapping. On the other hand, we found little agreement between simulations and experimental data when considering distortions in the stimuli’s spectrum. This mismatch is probably related on how the auditory front-ends combined various spatial cues. We will discuss these results and further extensions of the framework to match actual performances in diverse acoustic scenarios.},
	booktitle = {In {Proc}.  of the {DAGA} 2021, 47th {Annual} {Conference} on {Acoustics}},
	author = {Barumerli, R. and Majdak, P. and Baumgartner, R. and Geronazzo, M. and Avanzini, F.},
	month = aug,
	year = {2021},
}

@inproceedings{barumerli_evaluation_2020,
	address = {Lyon},
	title = {Evaluation of a human sound localization model based on {Bayesian} inference},
	booktitle = {Proc. {Forum} {Acusicum} 2020},
	author = {Barumerli, R. and Majdak, P. and Baumgartner, R. and Geronazzo, M. and Avanzini, F.},
	month = apr,
	year = {2020},
}

@article{bahadori_action_2019-1,
	series = {{SISMES} {XI} {NATIONAL} {CONGRESS}, {Supplement} 1},
	title = {Action anticipation for different sounds’ semantics},
	volume = {15},
	issn = {1825-1234},
	url = {https://doi.org/10.1007/s11332-019-00578-6},
	doi = {10.1007/s11332-019-00578-6},
	language = {en},
	number = {1},
	urldate = {2019-10-29},
	journal = {Sport Sciences for Health},
	author = {Bahadori, M. and Barumerli, R. and Geronazzo, M. and Cecco, C. and Passarin, M. and Carner, M. and Cesari, P.},
	month = sep,
	year = {2019},
	pages = {1--117},
}

@inproceedings{geronazzo_evaluation_2019,
	address = {Aachen, DE},
	title = {On the evaluation of head-related transfer functions with probabilistic auditory models of human sound localization},
	isbn = {978-3-939296-15-7},
	abstract = {Understanding spatial hearing leads to implement efﬁcient and effective auralization rendering algorithms with headphones. Two important aspects contribute to sound localization: (i) acoustic ﬁltering of listener body, and (ii) non-acoustic factors introduced by auditory periphery. Accordingly, head-related transfer functions (HRTFs) describe users acoustics in terms of their spatial ﬁltering. Binaural synthesis through generic HRTFs (commonly a dummy head) is the most simple solution for an auralization framework. In this scenario, a high variability in localization tasks between subjects yields to an unreliable rendering. Listener’s acoustic and perceptual characterization require HRTF modeling and auditory models predictions in order to provide an effective auralization on individual basis. Systemic comparisons of HRTF approximations and different user proﬁles can help to predict listener’s performances. We consider a case study on both vertical and horizontal localization with different HRTFs and two probabilistic auditory models. In our analysis, spatial audio rendering with non-individual HRTFs has a special attention for its commercial relevance compared to unpractical and questionable use of individual HRTFs.},
	booktitle = {In {Proc}. 23rd {International} {Congress} on {Acoustics}},
	author = {Geronazzo, Michele and Barumerli, Roberto and Avanzini, Federico},
	month = sep,
	year = {2019},
	pages = {7686--7693},
}

@inproceedings{barumerli_auditory_2019,
	address = {Aachen, DE},
	title = {Auditory models comparison for horizontal localization of concurrent speakers in adverse acoustic scenarios},
	isbn = {978-3-939296-15-7},
	abstract = {This paper aims at comparing and reproducing the predictions of two public available computational auditory models for speaker localization in different simulated environments. The direction-of-arrival (DOA) of sound sources in the horizontal plane can be extracted by using binaural spatial cues from room and user acoustics. Since our predictions consider the speciﬁcity of both models at the level of peripheral processing, the proposed solution for DOA extraction also provides a common multi-conditional training for the Gaussian Mixture Model (GMM) approach. A set of acoustic simulations of adverse conditions (i.e. multi speakers or high reverberant scenarios) supports the evaluation phase on robustness of the synthetic auditory process. Our analysis reproduces two case studies from the scientiﬁc literature in order to investigate the reliability of localization predictions in the frontal horizontal plane. Finally, a newly deﬁned acoustic scenario allows to identify differences between auditory models outcome in the entire horizontal plane. The results show a good agreement with previous literature and our machine learning approach emphasizes peculiarities of each approach for auditory peripheral processing.},
	booktitle = {In {Proc}. 23rd {International} {Congress} on {Acoustics}},
	author = {Barumerli, Roberto and Almenari, Andrea and Geronazzo, Michele and Di Nunzio, Giorgio Maria and Avanzini, Federico},
	month = sep,
	year = {2019},
	pages = {7686--7693},
}

@inproceedings{geronazzo_hoba-vr:_2018,
	address = {Milano},
	title = {Hoba-vr: hrtf on demand for binaural audio in immersive virtual reality environments},
	shorttitle = {Hoba-vr},
	url = {http://www.aes.org/e-lib/browse.cfm?elib=19546},
	abstract = {One of the main challenges of spatial audio rendering in headphones is the personalization of the so-called head-related transfer functions (HRTFs). HRTFs capture the listener's acoustic effects supporting immersive and realistic virtual reality (VR) contexts. This e-brief presents the HOBA-VR framework that provides a full-body VR experience with personalized HRTFs that were individually selected on demand based on anthropometric data (pinnae shapes). The proposed WAVH transfer format allows...},
	language = {English},
	urldate = {2018-06-12},
	publisher = {Audio Engineering Society},
	author = {Geronazzo, Michele and Kleimola, Jari and Sikström, Erik and de Götzen, Amalia and Serafin, Stefania and Avanzini, Federico},
	month = may,
	year = {2018},
}

@inproceedings{cesari_when_2018,
	address = {Torino, Italy},
	title = {When sounds convey emotions: sound localization and action pre-planning},
	abstract = {Understanding how people plan their action in relation to a detection of an external sound source is the main aim of this experiment. Several evidences corroborate the strict link between sound and action supporting the idea of the motor system involvement during sound perception. In this research we focus our attention on how individuals react to sounds that reach the space close to their body, the so-called peri-personal space (PPS). Thirty-six individuals were tested and by means of kinematics (MX Ultranet Vicon) and EMG (zero wire System) systems we extracted the individual premotor reaction time to quantify movement planning and action preparation. Subjects were equipped with Hefio headphone individually calibrated for listening to sounds selected from the International Affective Digitized Sounds (IADS) inducing positive negative and neutral
emotions. Participants were instructed to keep a standing posture while listening to the sounds having looming characteristics and to raise their arms as fast as possible once the sound stopped.
The analysis considered 5 sounds (2 pleasant 2 unpleasant and 1 neutral) stopping at 5 different
distances. After listening to each sound they were instructed to indicate with their right index finger on their left upper arm extended forward where the sound stopped (distance estimation). Premotor reaction time modulated significantly with the distances at which the sounds stopped with an
accuracy of few centimeters. Individuals were systematically faster at each distance when reacting to a neutral sound when compared to sounds carrying semantics. For distance estimation
individuals were highly precise in locating the distances while the neutral sound was systematically
perceived as the closest at each distance when compared with the semantic sounds. The results
evidenced the role of sound semantic decoding in action preparation and localization.},
	booktitle = {{MeeTo} – {From} moving bodies to interactive minds},
	author = {Cesari, Paola and Geronazzo, Michele},
	month = may,
	year = {2018},
}

@inproceedings{gabrieli_cinematica_2017,
	address = {Genova},
	title = {La cinematica del rachide durante l'estricazione: protocollo di ricerca e studio di fattibilità},
	author = {Gabrieli, A. and Nardello, F. and Liberto, M. and Geronazzo, M. and Arcozzi, D. and Adami, E.C. and Cesari, P. and Polati, E. and Geat, E. and Valoti, O. and Zamparo, P.},
	month = oct,
	year = {2017},
}

@techreport{baldovino_audio_2016,
	address = {Aalborg, DK},
	title = {Audio augmented reality headset: a product requirements research in today’s available technologies},
	institution = {Audio Engineering Society},
	author = {Baldovino, Guido and Geronazzo, Michele},
	month = aug,
	year = {2016},
}

@inproceedings{scaiella_subjective_2015,
	address = {Florence, Italy},
	title = {Subjective evaluation of a low-order parametric filter model of the pinna for binaural sound rendering},
	booktitle = {22nd {Int}. {Congress} on {Sound} and {Vibration} ({ICSV22})},
	author = {Scaiella, Sandro and Spagnol, Simone and Geronazzo, Michele and Avanzini, Federico},
	month = jul,
	year = {2015},
}

@misc{geronazzo_msm_2020,
	title = {{MSM} binsdn -- binaural synthesis and scattering delay networks, https://github.com/msmhrtf/binsdn - a {Unity} plugin},
	url = {https://github.com/msmhrtf/binsdn},
	author = {Geronazzo, Michele and Tissieres, Jason Yves},
	year = {2020},
}

@misc{geronazzo_msm_2019,
	title = {{MSM} sel -- {HRTF} selection tool, https://github.com/msmhrtf/sel - a {Matlab} framework for {HRTF} personalization},
	url = {https://github.com/msmhrtf/sel},
	author = {Geronazzo, Michele and Bedin, Alberto and Peruch, Enrico and Prandoni, Fabio},
	year = {2019},
}

@misc{geronazzo_hoba_2015,
	title = {{HOBA} -- {Hrtfs} {On}-demand for {Binaural} {Audio}, https://github.com/hoba3d - a web framework for personalized {3D} audio rendering},
	url = {https://github.com/hoba3d},
	author = {Geronazzo, Michele and Kleimola, Jari},
	year = {2015},
}

@misc{geronazzo_sofa_2014,
	title = {{SOFA} -- {Spatially} {Oriented} {Format} for {Acoustics}, http://www.sofaconventions.org/, headphone support for standardization},
	url = {http://www.sofaconventions.org/},
	author = {Geronazzo, Michele},
	year = {2014},
}

@misc{geronazzo_bt-dei_2014,
	title = {{BT}-{DEI} {HpIRs} in http://padva.dei.unipd.it , a public headphone impulse response database},
	url = {http://padva.dei.unipd.it/?page_id=345},
	author = {Geronazzo, Michele},
	year = {2014},
}

@phdthesis{geronazzo_mixed_2014,
	address = {Padova, Italy},
	type = {Ph.{D}. {Thesis}},
	title = {Mixed structural models for {3D} audio in virtual environments},
	school = {University of Padova},
	author = {Geronazzo, Michele},
	month = apr,
	year = {2014},
}
